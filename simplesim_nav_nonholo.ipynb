{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoxOjIlOImwx"
      },
      "source": [
        "# SimpleSim Non-Holonomic Navigation Challenge\n",
        "\n",
        "This notebook attempts to train an agent solve a simplesim non-holonomic driving navigation problem with 1 target with random spawn location\n",
        "\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp8rSS4DIhEV",
        "outputId": "5c0b7f03-662c-41c5-b3f9-f470e27754e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3[extra]>=2.0.0a4\n",
            "  Downloading stable_baselines3-2.1.0-py3-none-any.whl (178 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/178.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m153.6/178.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra]>=2.0.0a4)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.0.1+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.5.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.13.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (13.5.2)\n",
            "Collecting shimmy[atari]~=1.1.0 (from stable-baselines3[extra]>=2.0.0a4)\n",
            "  Downloading Shimmy-1.1.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra]>=2.0.0a4)\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a4) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a4) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a4)\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a4) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a4)\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.1.0->stable-baselines3[extra]>=2.0.0a4)\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a4) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a4) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a4) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a4) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a4) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->stable-baselines3[extra]>=2.0.0a4) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->stable-baselines3[extra]>=2.0.0a4) (16.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]>=2.0.0a4) (2023.3.post1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.1.0->stable-baselines3[extra]>=2.0.0a4) (6.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a4) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a4) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a4) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a4) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a4) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]>=2.0.0a4) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=632a0d9adc9c81d10edfde71452428eedcc8efe575b1212169db9c51598f01b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy, AutoROM.accept-rom-license, autorom, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-1.1.0 stable-baselines3-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0wXGn1ylGFR"
      },
      "source": [
        "### Setup Tensorboard Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X2BNcPi7lID_"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqxatIwPOXe_"
      },
      "source": [
        "##  Custom Gym Envs\n",
        "\n",
        "Below are a couple of simpler lower order gridworld type gym environments that can be used as testing and debugging examples ,as well as our main SimpleSIm non-holonomic driving environment (which is imported from the seperate source files env.py and env_gym.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5SNFv8TF58R0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYzDXA9vJfz1",
        "outputId": "f562a0de-9a0d-486e-fd16-06851997d502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class GoLeftEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Environment that follows gym interface.\n",
        "    This is a simple env where the agent must learn to go always left.\n",
        "    \"\"\"\n",
        "\n",
        "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "    metadata = {\"render_modes\": [\"console\"]}\n",
        "\n",
        "    # Define constants for clearer code\n",
        "    LEFT = 0\n",
        "    RIGHT = 1\n",
        "\n",
        "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
        "        super(GoLeftEnv, self).__init__()\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        # Size of the 1D-grid\n",
        "        self.grid_size = grid_size\n",
        "        # Initialize the agent at the right of the grid\n",
        "        self.agent_pos = grid_size - 1\n",
        "\n",
        "        # Define action and observation space\n",
        "        # They must be gym.spaces objects\n",
        "        # Example when using discrete actions, we have two: left and right\n",
        "        n_actions = 2\n",
        "        self.action_space = spaces.Discrete(n_actions)\n",
        "        # The observation will be the coordinate of the agent\n",
        "        # this can be described both by Discrete and Box space\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=self.grid_size-1, shape=(1,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Important: the observation must be a numpy array\n",
        "        :return: (np.array)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed, options=options)\n",
        "        # Initialize the agent at the right of the grid\n",
        "        self.agent_pos = self.grid_size - 1\n",
        "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "        return np.array([self.agent_pos]).astype(np.float32), {}  # empty info dict\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == self.LEFT:\n",
        "            self.agent_pos -= 1\n",
        "        elif action == self.RIGHT:\n",
        "            self.agent_pos += 1\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Received invalid action={action} which is not part of the action space\"\n",
        "            )\n",
        "\n",
        "        # Account for the boundaries of the grid\n",
        "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size-1)\n",
        "\n",
        "        # Are we at the left of the grid?\n",
        "        terminated = bool(self.agent_pos == 0)\n",
        "        truncated = False  # we do not limit the number of steps here\n",
        "\n",
        "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "        reward = 1 if self.agent_pos == 0 else 0\n",
        "\n",
        "        # Optionally we can pass additional info, we are not using that for now\n",
        "        info = {}\n",
        "\n",
        "        return (\n",
        "            np.array([self.agent_pos]).astype(np.float32),\n",
        "            reward,\n",
        "            terminated,\n",
        "            truncated,\n",
        "            info,\n",
        "        )\n",
        "\n",
        "    def render(self):\n",
        "        # agent is represented as a cross, rest as a dot\n",
        "        if self.render_mode == \"console\":\n",
        "            print(\".\" * self.agent_pos, end=\"\")\n",
        "            print(\"x\", end=\"\")\n",
        "            print(\".\" * ((self.grid_size - self.agent_pos)-1))\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "class GoDownLeftEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Environment that follows gym interface.\n",
        "    This is a simple env where the agent must learn to go always left.\n",
        "    \"\"\"\n",
        "\n",
        "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "    metadata = {\"render_modes\": [\"console\"]}\n",
        "\n",
        "    # Define constants for clearer code\n",
        "    NOTHING = 0\n",
        "    RIGHT = 1\n",
        "    UP = 2\n",
        "    LEFT = 3\n",
        "    DOWN = 4\n",
        "\n",
        "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
        "        super(GoDownLeftEnv, self).__init__()\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        # Size of the 1D-grid\n",
        "        self.grid_size = grid_size\n",
        "        # Initialize the agent at the right of the grid\n",
        "        self.agent_pos_x = grid_size - 1\n",
        "        self.agent_pos_y = grid_size - 1\n",
        "\n",
        "        # Define action and observation space\n",
        "        # They must be gym.spaces objects\n",
        "        # Example when using discrete actions, we have two: left and right\n",
        "        n_actions = 5\n",
        "        self.action_space = spaces.Discrete(n_actions)\n",
        "        # The observation will be the x and y coordinates of the agent\n",
        "        # this can be described both by Discrete and Box space\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=self.grid_size-1, shape=(2,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Important: the observation must be a numpy array\n",
        "        :return: (np.array)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed, options=options)\n",
        "        # Initialize the agent at the right of the grid\n",
        "        self.agent_pos_x = self.grid_size - 1\n",
        "        self.agent_pos_y = self.grid_size - 1\n",
        "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "        return np.array([self.agent_pos_x, self.agent_pos_y]).astype(np.float32), {}  # empty info dict\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == self.NOTHING:\n",
        "          # Do nothing\n",
        "          pass\n",
        "        elif action == self.RIGHT:\n",
        "            self.agent_pos_x += 1\n",
        "        elif action == self.UP:\n",
        "            self.agent_pos_y += 1\n",
        "        elif action == self.LEFT:\n",
        "            self.agent_pos_x -= 1\n",
        "        elif action == self.DOWN:\n",
        "            self.agent_pos_y -= 1\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Received invalid action={action} which is not part of the action space\"\n",
        "            )\n",
        "\n",
        "        # Account for the boundaries of the grid\n",
        "        self.agent_pos_x = np.clip(self.agent_pos_x, 0, self.grid_size-1)\n",
        "        self.agent_pos_y = np.clip(self.agent_pos_y, 0, self.grid_size-1)\n",
        "\n",
        "        # Are we at the left of the grid?\n",
        "        terminated = bool(self.agent_pos_x == 0 and self.agent_pos_y == 0)\n",
        "        truncated = False  # we do not limit the number of steps here\n",
        "\n",
        "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "        reward = 1 if (self.agent_pos_x == 0 and self.agent_pos_y == 0) else 0\n",
        "\n",
        "        # Optionally we can pass additional info, we are not using that for now\n",
        "        info = {}\n",
        "\n",
        "        return (\n",
        "            np.array([self.agent_pos_x, self.agent_pos_y]).astype(np.float32),\n",
        "            reward,\n",
        "            terminated,\n",
        "            truncated,\n",
        "            info,\n",
        "        )\n",
        "\n",
        "    def render(self):\n",
        "        # agent is represented as a cross, rest as a dot\n",
        "        if self.render_mode == \"console\":\n",
        "            for row in reversed(range(self.grid_size)):\n",
        "                print(\".\" * self.agent_pos_x, end=\"\")\n",
        "                if (row == self.agent_pos_y):\n",
        "                    print(\"x\", end=\"\")\n",
        "                else:\n",
        "                    print(\".\", end=\"\")\n",
        "                print(\".\" * ((self.grid_size - self.agent_pos_x)-1))\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "# Import our main environment\n",
        "from env_gym import SimpleSimGym\n",
        "STARTING_BUDGET = 200\n",
        "NUM_TARGETS = 1\n",
        "PLAYER_FOV = 60\n",
        "RENDER_MODE = \"rgb_array\"\n",
        "\n",
        "\n",
        "# This simple toggle can be used to switch which environment we are training the notebook on\n",
        "env_mode = 2  # 0 for GoLeft, 1 for GoDownLeft, 2 for SimpleSimGym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy5mlho1-Ine"
      },
      "source": [
        "### Validate the environment\n",
        "\n",
        "Stable Baselines3 provides a [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH-Hxz7H53hl",
        "outputId": "92fb1484-c081-46bb-a0a3-f67e4f491d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.env_checker import check_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CcUVatq-P0l",
        "outputId": "bb62c83a-ab64-49d4-ae7a-a367a4f2f23a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "if env_mode == 0:\n",
        "  env = GoLeftEnv()\n",
        "elif env_mode == 1:\n",
        "  env = GoDownLeftEnv()\n",
        "else:\n",
        "  env = SimpleSimGym(starting_budget=STARTING_BUDGET, num_targets=NUM_TARGETS, player_fov=PLAYER_FOV, render_mode=None)\n",
        "\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adsKMvDkRUn0"
      },
      "source": [
        "## Setup Callbacks\n",
        "\n",
        "### Auto Saving of the Best Model Callback\n",
        "\n",
        "Using the monitoring wrapper, we can save statistics of the environment, and use them to determine the mean training reward. This allows us to save the best model while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x_2jApX051gi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nzMHj7r3h78m"
      },
      "outputs": [],
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, check_freq, log_dir, save_dir, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(save_dir, \"best_model\")\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.log_dir is not None:\n",
        "            os.makedirs(self.log_dir, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "            # Retrieve training reward\n",
        "            # print(self.log_dir)\n",
        "            # print(load_results(self.log_dir))\n",
        "            # print(ts2xy(load_results(self.log_dir), \"timesteps\"))\n",
        "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
        "            if len(x) > 0:\n",
        "                # Mean training reward over the last 100 episodes\n",
        "                mean_reward = np.mean(y[-100:])\n",
        "                if self.verbose > 0:\n",
        "                    print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
        "                    print(\n",
        "                        \"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(\n",
        "                            self.best_mean_reward, mean_reward\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                # New best model, you could save the agent here\n",
        "                if mean_reward > self.best_mean_reward:\n",
        "                    self.best_mean_reward = mean_reward\n",
        "                    # Example for saving best model\n",
        "                    # if self.verbose > 0:\n",
        "                    print(\"Saving new best model at {} timesteps\".format(x[-1]))\n",
        "                    print(\"Saving new best model to {}.zip\".format(self.save_path))\n",
        "                    self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Curriculum Learning Callback\n",
        "\n",
        "Create a callback to slowly move the locations of the targets further away from the robot to increase early reward feedback for fast learning, and then increase the complexity later."
      ],
      "metadata": {
        "id": "WndGNiRKIIJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adaptive Training Rate Callback\n",
        "\n",
        "Reduces the learning rate over time to reduce instability in the later stages of training."
      ],
      "metadata": {
        "id": "-Y8Xe4S4IhvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_schedule(initial_value: float):\n",
        "    \"\"\"\n",
        "    Linear learning rate schedule.\n",
        "\n",
        "    :param initial_value: Initial learning rate.\n",
        "    :return: schedule that computes\n",
        "      current learning rate depending on remaining progress\n",
        "    \"\"\"\n",
        "    def func(progress_remaining: float) -> float:\n",
        "        \"\"\"\n",
        "        Progress will decrease from 1 (beginning) to 0.\n",
        "\n",
        "        :param progress_remaining:\n",
        "        :return: current learning rate\n",
        "        \"\"\"\n",
        "        return progress_remaining * initial_value\n",
        "\n",
        "    return func"
      ],
      "metadata": {
        "id": "hJ1h8P5zIhfX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3khFtkSE0g"
      },
      "source": [
        "### Baseline Testing the Environment\n",
        "\n",
        "Test the performance of an untrained (random) policy on the environment so that we can get a baseline performance to compare to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i62yf2LvSAYY"
      },
      "outputs": [],
      "source": [
        "# if env_mode == 0:\n",
        "#   env = GoLeftEnv(grid_size=10)\n",
        "# elif env_mode == 1:\n",
        "#   env = GoDownLeftEnv(grid_size=10)\n",
        "# else:\n",
        "#   env = SimpleSimGym(starting_budget=STARTING_BUDGET, num_targets=NUM_TARGETS, player_fov=PLAYER_FOV, render_mode=None)\n",
        "\n",
        "# obs, _ = env.reset()\n",
        "# env.render()\n",
        "\n",
        "# print(env.observation_space)\n",
        "# print(env.action_space)\n",
        "# print(env.action_space.sample())\n",
        "\n",
        "# if env_mode == 0:\n",
        "#   GO_LEFT = 0\n",
        "#   # Hardcoded best agent: always go left!\n",
        "#   n_steps = 20\n",
        "#   for step in range(n_steps):\n",
        "#       print(f\"Step {step + 1}\")\n",
        "#       obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
        "#       done = terminated or truncated\n",
        "#       print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "#       env.render()\n",
        "#       if done:\n",
        "#           print(\"Goal reached!\", \"reward=\", reward)\n",
        "#           break\n",
        "# else:\n",
        "#   GO_LEFT = 3\n",
        "#   GO_DOWN = 4\n",
        "#   # Hardcoded best agent: always go left!\n",
        "#   n_steps = 20\n",
        "#   for step in range(n_steps):\n",
        "#       # Go left\n",
        "#       print(f\"Step {step + 1}\")\n",
        "#       obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
        "#       done = terminated or truncated\n",
        "#       print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "#       env.render()\n",
        "#       # Then, go down\n",
        "#       print(f\"Step {step + 1}\")\n",
        "#       obs, reward, terminated, truncated, info = env.step(GO_DOWN)\n",
        "#       done = terminated or truncated\n",
        "#       print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "#       env.render()\n",
        "\n",
        "#       if done:\n",
        "#           print(\"Goal reached!\", \"reward=\", reward)\n",
        "#           break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv1e1qJETfHU"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5fbrQWC8G103"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"policy\": 'MlpPolicy',\n",
        "    \"total_timesteps\": 200_000,\n",
        "    \"logdir\": \"logs/\",\n",
        "    \"savedir\": \"saved_models/\"\n",
        "}\n",
        "# steps_per_curriculum = config[\"total_timesteps\"] // 20\n",
        "\n",
        "# Create log dir\n",
        "os.makedirs(config[\"logdir\"], exist_ok=True)\n",
        "\n",
        "# Create save dir\n",
        "os.makedirs(config[\"savedir\"], exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PQfLBE28SNDr"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# Instantiate and wrap the env\n",
        "if env_mode == 0:\n",
        "  env = make_vec_env(GoLeftEnv, n_envs=1, monitor_dir=config[\"logdir\"], env_kwargs=dict(grid_size=10))\n",
        "elif env_mode == 1:\n",
        "  env = make_vec_env(GoDownLeftEnv, n_envs=1, monitor_dir=config[\"logdir\"], env_kwargs=dict(grid_size=10))\n",
        "else:\n",
        "  env = make_vec_env(SimpleSimGym, n_envs=1, monitor_dir=config[\"logdir\"], env_kwargs=dict(starting_budget=STARTING_BUDGET, num_targets=NUM_TARGETS, player_fov=PLAYER_FOV, render_mode=RENDER_MODE))\n",
        "\n",
        "\n",
        "# Setup callbacks\n",
        "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=config[\"logdir\"], save_dir=config[\"savedir\"], verbose=0)\n",
        "\n",
        "# curriculum = np.linspace(0, 1, num=(int(config[\"total_timesteps\"]/steps_per_curriculum))) # increase from 5 to farthest corner distance\n",
        "# print(curriculum)\n",
        "# learning_rate = np.linspace(50, (1.5*500)//2, num=(config[\"total_timesteps\"]//steps_per_curriculum)) # increase from 5 to farthest corner distance\n",
        "\n",
        "model = DQN(config[\"policy\"], env, tensorboard_log=config[\"logdir\"], verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curriculum Training"
      ],
      "metadata": {
        "id": "dIGZqeNXmJJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# i = 0\n",
        "# timesteps_trained = 0\n",
        "# while timesteps_trained < config[\"total_timesteps\"]:\n",
        "#     # model.learning_rate = learning_rate[i]\n",
        "#     env.set_attr(\"game.curriculum\", curriculum[i]) # up the env difficulty\n",
        "#     env.reset()\n",
        "#     print(f\"Step {timesteps_trained}: curriculum={env.get_attr('game.curriculum')}, learning_rate={model.learning_rate}\", flush=True)\n",
        "\n",
        "#     if i < 8:\n",
        "#       model.learn(steps_per_curriculum, tb_log_name=\"curriculum_learning\", progress_bar=True, reset_num_timesteps=False)\n",
        "#     else:\n",
        "#       # Only auto save from near the end otherwise we might get a policy only good at close targets\n",
        "#       model.learn(steps_per_curriculum, tb_log_name=\"curriculum_learning\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
        "\n",
        "#     timesteps_trained += steps_per_curriculum\n",
        "#     i += 1\n",
        "\n",
        "# print(i)"
      ],
      "metadata": {
        "id": "OT_kKp2Ql_Js"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal Training"
      ],
      "metadata": {
        "id": "xcNl-vXgmHgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the agent\n",
        "model.learn(config[\"total_timesteps\"], tb_log_name=\"DQN\", callback=auto_save_callback, progress_bar=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294,
          "referenced_widgets": [
            "2bdf01d2ef434f29a948a4940625f6b0",
            "3bc1dfa18bf14d2a97852ff75ecad0f9"
          ]
        },
        "id": "6WOtrw-xmGhc",
        "outputId": "e0e86910-3357-457d-c95a-9bb833c3ae35"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bdf01d2ef434f29a948a4940625f6b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model at 842 timesteps\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model at 842 timesteps\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model to saved_models/best_model.zip\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to saved_models/best_model.zip\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model at 3820 timesteps\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model at 3820 timesteps\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model to saved_models/best_model.zip\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to saved_models/best_model.zip\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model at 5905 timesteps\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model at 5905 timesteps\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model to saved_models/best_model.zip\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to saved_models/best_model.zip\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model at 6973 timesteps\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model at 6973 timesteps\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model to saved_models/best_model.zip\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to saved_models/best_model.zip\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model at 137966 timesteps\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model at 137966 timesteps\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model to saved_models/best_model.zip\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to saved_models/best_model.zip\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model at 172867 timesteps\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model at 172867 timesteps\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model to saved_models/best_model.zip\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to saved_models/best_model.zip\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model at 174863 timesteps\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model at 174863 timesteps\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving new best model to saved_models/best_model.zip\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to saved_models/best_model.zip\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x7c03e74b0340>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w9ifee98kOL"
      },
      "source": [
        "## Continue Training or Run Dupliacte Experiments?\n",
        "\n",
        "This cell can be used to either continue training on an existing model (use `reset_num_timesteps=False`) or to run additional duplicate experiments training from scratch to test training consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89,
          "referenced_widgets": [
            "e14c6d372ee347f7be51d1027de08037",
            "af9946047ab64b4d9d91550f4f271c32"
          ]
        },
        "id": "kBTmEWCf8jlF",
        "outputId": "51004445-a01e-4697-96f2-9a2c94d0e934"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e14c6d372ee347f7be51d1027de08037"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# # Run additional duplicate experiments from scratch to test training consistency\n",
        "model2 = PPO(config[\"policy\"], env, tensorboard_log=config[\"logdir\"], verbose=0)\n",
        "model2.learn(config[\"total_timesteps\"], tb_log_name=\"PPO\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=True)\n",
        "\n",
        "# model3 = A2C(config[\"policy\"], env, tensorboard_log=config[\"logdir\"], verbose=0)\n",
        "# model3.learn(config[\"total_timesteps\"], callback=auto_save_callback, progress_bar=True, reset_num_timesteps=True)\n",
        "\n",
        "# model.learn(200_000, tb_log_name=\"run\", reset_num_timesteps=False)\n",
        "# model.learn(200_000, tb_log_name=\"run\", reset_num_timesteps=False)\n",
        "# model.learn(200_000, tb_log_name=\"run\", reset_num_timesteps=False)\n",
        "# model.learn(200_000, tb_log_name=\"run\", reset_num_timesteps=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNtnuSaSAM8B"
      },
      "source": [
        "### Load Model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrtqc8GcAOT_"
      },
      "outputs": [],
      "source": [
        "# model = A2C.load(f\"{config['savedir']}/{config['policy']}\")\n",
        "\n",
        "# Load the best model\n",
        "best_model = A2C.load(f\"{config['savedir']}/best_model.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaEjy8R3_JoP"
      },
      "source": [
        "### Save Model ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9wH6IE__MTk"
      },
      "outputs": [],
      "source": [
        "# The model will be saved under MlpPolicy.zip\n",
        "model.save(f\"{config['savedir']}/{config['policy']}\")\n",
        "# model.save(f\"{config['savedir']}/{config['policy']}2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmb5J448ly20"
      },
      "source": [
        "# Show Tensorboard Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiVm9WoqlyLf"
      },
      "outputs": [],
      "source": [
        "# Open Tensorboard Logging\n",
        "%tensorboard --logdir logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBOWRrb_wmyx"
      },
      "source": [
        "### Check Performance\n",
        "\n",
        "Check if the policy can consistently succeed in the environment over multilpe episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBLWeFCdmGEE"
      },
      "outputs": [],
      "source": [
        "# Instantiate the eval env\n",
        "if env_mode == 0:\n",
        "    eval_env = make_vec_env(GoLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))\n",
        "elif env_mode == 1:\n",
        "    eval_env = make_vec_env(GoDownLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))\n",
        "else:\n",
        "    eval_env = make_vec_env(SimpleSimGym, n_envs=1, env_kwargs=dict(starting_budget=STARTING_BUDGET, num_targets=NUM_TARGETS, player_fov=PLAYER_FOV, render_mode=RENDER_MODE))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# # Check performance of best vs last model\n",
        "# models = {\"last\": model, \"best\": best_model}\n",
        "\n",
        "# for key in models.keys():\n",
        "#     eval_model = models[key]\n",
        "\n",
        "#     # Instantiate the eval env\n",
        "#     if env_mode == 0:\n",
        "#         eval_env = make_vec_env(GoLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))\n",
        "#     elif env_mode == 1:\n",
        "#         eval_env = make_vec_env(GoDownLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))\n",
        "#     else:\n",
        "#         eval_env = make_vec_env(SimpleSimGym, n_envs=1, env_kwargs=dict(starting_budget=STARTING_BUDGET, num_targets=NUM_TARGETS, player_fov=PLAYER_FOV, render_mode=RENDER_MODE))\n",
        "\n",
        "\n",
        "#     # Test average reward over multiple episodes\n",
        "#     mean_reward, std_reward = evaluate_policy(eval_model, eval_env, n_eval_episodes=50)\n",
        "#     print(f\"MODEL TYPE: {key}\")\n",
        "#     print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6kivS0SwXba"
      },
      "source": [
        "### Visualize Policy with Printouts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model = best_model"
      ],
      "metadata": {
        "id": "jdVbETzefVhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xevkAQWN47xK"
      },
      "outputs": [],
      "source": [
        "if env_mode == 0 or env_mode == 1:\n",
        "    # Test how many times it successfully reaches the end in 20 steps\n",
        "    eval_steps = 20\n",
        "    successes = 0\n",
        "    for i in range(10):\n",
        "      obs = eval_env.reset()\n",
        "      for step in range(eval_steps):\n",
        "          action, _ = eval_model.predict(obs, deterministic=True)\n",
        "          # print(f\"Step {step + 1}\")\n",
        "          # print(\"Action: \", action)\n",
        "          obs, reward, done, info = eval_env.step(action)\n",
        "          # print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "          # eval_env.render()\n",
        "          if done:\n",
        "              # Note that the VecEnv resets automatically\n",
        "              # when a done signal is encountered\n",
        "              # print(\"Goal reached!\", \"reward=\", reward, \"\\n\")\n",
        "              successes += 1\n",
        "              break\n",
        "    print(\"======================================\")\n",
        "    print(f\"{successes} successes / 10 tries\")\n",
        "    print(\"======================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJbeiF0RUN-p"
      },
      "outputs": [],
      "source": [
        "if env_mode == 0 or env_mode == 1:\n",
        "  # Test the trained agent\n",
        "  # using the vecenv\n",
        "  obs = eval_env.reset()\n",
        "  print(f\"Step 0\")\n",
        "  print(\"obs=\", obs)\n",
        "  eval_env.render()\n",
        "  n_steps = 40\n",
        "  for step in range(n_steps):\n",
        "      action, _ = eval_model.predict(obs, deterministic=True)\n",
        "      print(f\"Step {step + 1}\")\n",
        "      print(\"Action: \", action)\n",
        "      obs, reward, done, info = eval_env.step(action)\n",
        "      print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "      eval_env.render()\n",
        "      if done:\n",
        "          # Note that the VecEnv resets automatically\n",
        "          # when a done signal is encountered\n",
        "          print(\"Goal reached!\", \"reward=\", reward)\n",
        "          break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFMA7kYCvjxQ"
      },
      "source": [
        "### Prepare Video Recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmbgVRvIvjxR"
      },
      "outputs": [],
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "def show_videos(video_path=\"\", prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "    :param video_path: (str) Path to the folder containing videos\n",
        "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "    \"\"\"\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "\n",
        "\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder #, DummyVecEnv\n",
        "\n",
        "# # Create videos dir\n",
        "# videos_dir = \"./videos/\"\n",
        "# os.makedirs(videos_dir, exist_ok=True)\n",
        "\n",
        "def record_video(eval_env, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
        "    \"\"\"\n",
        "    :param eval_env: (vec env)\n",
        "    :param model: (RL model)\n",
        "    :param video_length: (int)\n",
        "    :param prefix: (str)\n",
        "    :param video_folder: (str)\n",
        "    \"\"\"\n",
        "    # eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "    # Start the video at step=0 and record 500 steps\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder=video_folder,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix,\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "    # Close the video recorder\n",
        "    eval_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx_YDEcGvjxR"
      },
      "source": [
        "### Visualize Trained Agent with Video\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY9ncXp8vjxR"
      },
      "outputs": [],
      "source": [
        "if env_mode == 2:\n",
        "    record_video(eval_env, eval_model, video_length=500*10, prefix=\"a2c-simplesim\")\n",
        "    show_videos(\"videos\", prefix=\"a2c\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2bdf01d2ef434f29a948a4940625f6b0": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3bc1dfa18bf14d2a97852ff75ecad0f9",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m199,935/200,000 \u001b[0m [ \u001b[33m0:03:55\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m675 it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">199,935/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:03:55</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">675 it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "3bc1dfa18bf14d2a97852ff75ecad0f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e14c6d372ee347f7be51d1027de08037": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_af9946047ab64b4d9d91550f4f271c32",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m   5%\u001b[0m \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10,217/200,000 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:07:24\u001b[0m , \u001b[31m428 it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   5%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">10,217/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:25</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:07:24</span> , <span style=\"color: #800000; text-decoration-color: #800000\">428 it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "af9946047ab64b4d9d91550f4f271c32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}