# -*- coding: utf-8 -*-
"""DQN C51/Rainbow Tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/9_c51_tutorial.ipynb

##### Copyright 2023 The TF-Agents Authors.
"""

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# ---------------------------------------------------------------------------- #
#                                    IMPORTS                                   #
# ---------------------------------------------------------------------------- #
# import base64
# import imageio
# import IPython
# import matplotlib
# import pyvirtualdisplay
# from tf_agents.drivers import dynamic_step_driver
# from tf_agents.eval import metric_utils
# from tf_agents.metrics import tf_metrics

import matplotlib.pyplot as plt
import PIL.Image
import tensorflow as tf
from tf_agents.agents.categorical_dqn import categorical_dqn_agent
from tf_agents.environments import suite_gym
from tf_agents.environments import tf_py_environment
from tf_agents.networks import categorical_q_network
from tf_agents.policies import random_tf_policy
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.trajectories import trajectory
from tf_agents.utils import common
from tf_agents.specs import tensor_spec
import numpy as np
from SimpleSimGym import SimpleSimGym


# ---------------------------------------------------------------------------- #
#                                   FUNCTIONS                                  #
# ---------------------------------------------------------------------------- #
def getCustomPreprocessingLayer(obs_spec: tensor_spec.TensorSpec):
    """
    A custom preprocessing layer for the RL network that combines the environment
    observation spec variables.
    """
    inputs = {}
    features = {}
    tensors = {}
    specs = {}
    expected_dim = 0

    # Add each of the obs specs as inputs
    for name in obs_spec.keys():
        print(f"Spec Info:")
        # Get spec info
        spec = obs_spec[name]
        print(f"name: {name}")
        print(f"spec.shape: {spec.shape}")
        batch_size = 1
        try:
            state_dims = spec.shape[0]
        except:
            # If the space has the damn stupid numpy shape of "()"
            state_dims = 1
        print(f"state_dims: {state_dims}")
        input_shape = (state_dims, batch_size)
        print(f"input_shape: {input_shape}")
        data_type = spec.dtype
        print("")

        # Inputs
        inputs[name] = tf.keras.Input(shape=[state_dims], dtype=data_type, name=name)
        # Features
        features[name] = inputs[name]
        tensors[name] = tf.ones(input_shape, data_type)
        specs[name] = tensor_spec.TensorSpec([state_dims], data_type)
        expected_dim += state_dims

    # Final combine
    features = tf.keras.layers.concatenate(features.values(), axis=-1)
    preprocessing_combiner = tf.keras.Model(inputs=inputs, outputs=features)

    print("")

    return preprocessing_combiner

def compute_avg_return(environment: SimpleSimGym, policy, num_episodes=10):
    """
    Computes the average return for a given policy over a certain number of
    episodes. This is the core and classic loop of the RL training process.
    """

    total_return = 0.0

    print("\n================== Running training loop: ==================")
    for _ in range(num_episodes):
        print("ENV SPEC ORIGINAL:")
        print(environment.time_step_spec().observation["avg_conf"].shape)
        time_step = environment.reset()
        print("ENV SPECS AFTER RESET:")
        print(time_step.observation["avg_conf"].shape)

        episode_return = 0.0

        while not time_step.is_last():
            print(f"Current Time Step State:")
            print(time_step)
            print("")

            print("Policy time_step_spec Details:")
            print(policy.time_step_spec)
            
            # Get the agents action given the current timestep state
            # TODO: Fix the error in this line
            action_step = policy.action(time_step)
            print(f"action: {action_step}\n\n")

            time_step = environment.step(action_step.action)
            episode_return += time_step.reward.numpy()

        total_return += episode_return

    avg_return = total_return / num_episodes
    return avg_return.numpy()[0]

def collect_step(replay_buffer, environment, policy):
    """
    Set up the replay buffer and the initial data collection with the given policy
    """
    time_step = environment.current_time_step()
    action_step = policy.action(time_step)
    next_time_step = environment.step(action_step.action)
    traj = trajectory.from_transition(time_step, action_step, next_time_step)

    # Add trajectory to the replay buffer
    replay_buffer.add_batch(traj)

# def embed_mp4(filename):
#     """Embeds an mp4 file in the notebook."""
#     video = open(filename,'rb').read()
#     b64 = base64.b64encode(video)

#     tag = '''
#     <video width="640" height="480" controls>
#     <source src="data:video/mp4;base64,{0}" type="video/mp4">
#     Your browser does not support the video tag.
#     </video>'''.format(b64.decode())

#     return IPython.display.HTML(tag)


if __name__ == "__main__":
    # ---------------------------------------------------------------------------- #
    #                                Hyperparameters                               #
    # ---------------------------------------------------------------------------- #
    STARTING_BUDGET = 1000
    NUM_TARGETS = 8
    PLAYER_FOV = 90

    env_name = "ReLOaDSimpleEnv"  # @param {type:"string"}

    num_iterations = 15000  # @param {type:"integer"}

    initial_collect_steps = 1000  # @param {type:"integer"}
    collect_steps_per_iteration = 1  # @param {type:"integer"}
    replay_buffer_capacity = 100000  # @param {type:"integer"}

    fc_layer_params = (100,)

    batch_size = 64  # @param {type:"integer"}
    learning_rate = 1e-3  # @param {type:"number"}
    gamma = 0.99
    log_interval = 200  # @param {type:"integer"}

    num_atoms = 51  # @param {type:"integer"}
    min_q_value = -20  # @param {type:"integer"}
    max_q_value = 20  # @param {type:"integer"}
    n_step_update = 2  # @param {type:"integer"}

    num_eval_episodes = 10  # @param {type:"integer"}
    eval_interval = 1000  # @param {type:"integer"}

    # ---------------------------------------------------------------------------- #
    #                                  Environment                                 #
    # ---------------------------------------------------------------------------- #
    # train_py_env = suite_gym.load(env_name)
    # eval_py_env = suite_gym.load(env_name)
    train_py_env = SimpleSimGym(STARTING_BUDGET, NUM_TARGETS, PLAYER_FOV)
    eval_py_env = SimpleSimGym(STARTING_BUDGET, NUM_TARGETS, PLAYER_FOV)

    train_env = tf_py_environment.TFPyEnvironment(train_py_env)
    eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)

    # ---------------------------------------------------------------------------- #
    #                                     Agent                                    #
    # ---------------------------------------------------------------------------- #
    # To create a C51 Agent, we first need to create a `CategoricalQNetwork`. The
    # API of the `CategoricalQNetwork` is the same as that of the `QNetwork`,
    # except that there is an additional argument `num_atoms`. This represents
    # the number of support points in our probability distribution estimates.
    # (The above image includes 10 support points, each represented by a vertical
    # blue bar.) As you can tell from the name, the default number of atoms is
    # 51.
    preprocessing_combiner = getCustomPreprocessingLayer(train_env.observation_spec())

    categorical_q_net = categorical_q_network.CategoricalQNetwork(
        train_env.observation_spec(),
        train_env.action_spec(),
        num_atoms=num_atoms,
        fc_layer_params=fc_layer_params,
        preprocessing_combiner=preprocessing_combiner,
    )

    # We also need an `optimizer` to train the network we just created, and a
    # `train_step_counter` variable to keep track of how many times the network
    # was updated.
    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)

    train_step_counter = tf.Variable(0)

    agent = categorical_dqn_agent.CategoricalDqnAgent(
        train_env.time_step_spec(),
        train_env.action_spec(),
        categorical_q_network=categorical_q_net,
        optimizer=optimizer,
        min_q_value=min_q_value,
        max_q_value=max_q_value,
        n_step_update=n_step_update,
        td_errors_loss_fn=common.element_wise_squared_loss,
        gamma=gamma,
        train_step_counter=train_step_counter,
    )
    agent.initialize()

    # # Demo of getting the average return for a random policy
    # random_policy = random_tf_policy.RandomTFPolicy(
    #     train_env.time_step_spec(), train_env.action_spec()
    # )
    # compute_avg_return(eval_env, random_policy, num_eval_episodes)

    # ---------------------------------------------------------------------------- #
    #                                Data Collection                               #
    # ---------------------------------------------------------------------------- #
    """
    As in the DQN tutorial, set up the replay buffer and the initial data collection with the random policy.
    """
    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
        data_spec=agent.collect_data_spec,
        batch_size=train_env.batch_size,
        max_length=replay_buffer_capacity,
    )

    # # This loop is so common in RL, that we provide standard implementations of
    # # these. For more details see the drivers module.
    # for _ in range(initial_collect_steps):
    #     collect_step(replay_buffer, train_env, random_policy)

    # Dataset generates trajectories with shape [BxTx...] where
    # T = n_step_update + 1.
    dataset = replay_buffer.as_dataset(
        num_parallel_calls=3, sample_batch_size=batch_size, num_steps=n_step_update + 1
    ).prefetch(3)

    iterator = iter(dataset)

    # ---------------------------------------------------------------------------- #
    #                              Training the agent                              #
    # ---------------------------------------------------------------------------- #
    # Train by collecting data from the environment and optimizing the agent's networks.

    # (Optional) Optimize by wrapping some of the code in a graph using TF function.
    agent.train = common.function(agent.train)

    # Reset the train step
    agent.train_step_counter.assign(0)

    # Evaluate the agent's policy once before training.
    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
    returns = [avg_return]

    for _ in range(num_iterations):
        # Collect a few steps using collect_policy and save to the replay buffer.
        for _ in range(collect_steps_per_iteration):
            collect_step(train_env, agent.collect_policy)

        # Sample a batch of data from the buffer and update the agent's network.
        experience, unused_info = next(iterator)
        train_loss = agent.train(experience)

        step = agent.train_step_counter.numpy()

        if step % log_interval == 0:
            print("step = {0}: loss = {1}".format(step, train_loss.loss))

        if step % eval_interval == 0:
            avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
            print("step = {0}: Average Return = {1:.2f}".format(step, avg_return))
            returns.append(avg_return)

    # ---------------------------------------------------------------------------- #
    #                                 Visualization                                #
    # ---------------------------------------------------------------------------- #
    # Plot return vs global steps to see the performance of our agent (the
    # nvironment gives a reward of +1 for every time step the pole stays up, and
    # since the maximum number of steps is 500, the maximum possible return is
    # also 500.)
    steps = range(0, num_iterations + 1, eval_interval)
    plt.plot(steps, returns)
    plt.ylabel("Average Return")
    plt.xlabel("Step")
    plt.ylim(top=550)

    # # Visualise the agent's policy for a few episodes with a video
    # num_episodes = 3
    # video_filename = 'imageio.mp4'
    # with imageio.get_writer(video_filename, fps=60) as video:
    #   for _ in range(num_episodes):
    #     time_step = eval_env.reset()
    #     video.append_data(eval_py_env.render())
    #     while not time_step.is_last():
    #       action_step = agent.policy.action(time_step)
    #       time_step = eval_env.step(action_step.action)
    #       video.append_data(eval_py_env.render())
    # embed_mp4(video_filename)
