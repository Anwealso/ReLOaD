{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoxOjIlOImwx"
   },
   "source": [
    "# SimpleSim Non-Holonomic Navigation Challenge\n",
    "\n",
    "This notebook attempts to train an agent solve a simplesim non-holonomic driving navigation problem with 1 target with random spawn location.\n",
    "This time, however we are teaching the agent to dwell at the goal as well instead of simply ending the episode.\n",
    "\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sp8rSS4DIhEV",
    "outputId": "5c0b7f03-662c-41c5-b3f9-f470e27754e1"
   },
   "outputs": [],
   "source": [
    "# !pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0wXGn1ylGFR"
   },
   "source": [
    "### Setup Tensorboard Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2BNcPi7lID_"
   },
   "outputs": [],
   "source": [
    "# # Clear any logs from previous runs\n",
    "# !rm -rf ./logs/\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqxatIwPOXe_"
   },
   "source": [
    "##  Custom Gym Envs\n",
    "\n",
    "Below are a couple of simpler lower order gridworld type gym environments that can be used as testing and debugging examples ,as well as our main SimpleSIm non-holonomic driving environment (which is imported from the seperate source files env.py and env_gym.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SNFv8TF58R0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Import our main environment\n",
    "from env_gym import SimpleSimGym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy5mlho1-Ine"
   },
   "source": [
    "### Validate the environment\n",
    "\n",
    "Stable Baselines3 provides a [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kH-Hxz7H53hl",
    "outputId": "92fb1484-c081-46bb-a0a3-f67e4f491d93"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = SimpleSimGym(max_budget=500, max_targets=3, num_classes=10, player_fov=60)\n",
    "\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adsKMvDkRUn0"
   },
   "source": [
    "## Import Auto Saving of Best Model Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzMHj7r3h78m"
   },
   "outputs": [],
   "source": [
    "from utils import SaveOnBestTrainingRewardCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv1e1qJETfHU"
   },
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fbrQWC8G103"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, SAC, TD3\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Environment Parameters\n",
    "MAX_BUDGET = 400\n",
    "MAX_TARGETS = 5\n",
    "NUM_CLASSES = 10\n",
    "PLAYER_FOV = 30\n",
    "RENDER_MODE = \"rgb_array\"\n",
    "ACTION_FORMAT = \"continuous\"\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MlpPolicy',\n",
    "    \"total_timesteps\": 1_000_000,\n",
    "    \"logdir\": \"logs/\",\n",
    "    \"savedir\": \"saved_models/\",\n",
    "}\n",
    "\n",
    "# Create log dir\n",
    "os.makedirs(config[\"logdir\"], exist_ok=True)\n",
    "\n",
    "# Create save dir\n",
    "os.makedirs(config[\"savedir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gmb5J448ly20"
   },
   "source": [
    "# Show Tensorboard Logs\n",
    "Visualise the live logs on tensorboard as we train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Tensorboard Logging\n",
    "%tensorboard --logdir logs/ --reload_interval 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcNl-vXgmHgo"
   },
   "source": [
    "### Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89,
     "referenced_widgets": [
      "e14c6d372ee347f7be51d1027de08037",
      "af9946047ab64b4d9d91550f4f271c32"
     ]
    },
    "id": "kBTmEWCf8jlF",
    "outputId": "51004445-a01e-4697-96f2-9a2c94d0e934"
   },
   "outputs": [],
   "source": [
    "# # Instantiate and wrap the env\n",
    "# env_ppo = make_vec_env(SimpleSimGym, \n",
    "#                    n_envs=1, \n",
    "#                    monitor_dir=config[\"logdir\"], \n",
    "#                    env_kwargs=dict(\n",
    "#                        max_budget=MAX_BUDGET, \n",
    "#                        max_targets=MAX_TARGETS, \n",
    "#                        num_classes=NUM_CLASSES, \n",
    "#                        player_fov=PLAYER_FOV, \n",
    "#                        render_mode=RENDER_MODE, \n",
    "#                        action_format=ACTION_FORMAT))\n",
    "\n",
    "# # Setup callbacks\n",
    "# ppo_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=config[\"logdir\"], save_dir=config[\"savedir\"], verbose=0)\n",
    "# # ppo_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=config[\"logdir\"], save_dir=config[\"savedir\"], model_name=\"ppo\", verbose=0)\n",
    "\n",
    "# # # Load the model from checkpoint 2\n",
    "# # model_ppo = PPO.load(f\"{config['savedir']}/MlpPolicy_PPO_step2000000\", env_ppo)\n",
    "\n",
    "# # Create the agent\n",
    "# model_ppo = PPO(config[\"policy\"], env_ppo, tensorboard_log=config[\"logdir\"], verbose=0)\n",
    "\n",
    "# # # Train the agent\n",
    "# # model_ppo.learn(config[\"total_timesteps\"], tb_log_name=\"PPO\", callback=ppo_save_callback, progress_bar=True)\n",
    "# # model_ppo.save(f\"{config['savedir']}/{config['policy']}_PPO\")\n",
    "\n",
    "# # Train in tranches\n",
    "# num_tranches = 10\n",
    "# for i in range(1, num_tranches+1):\n",
    "#     print(f\"Tranch {i}/{num_tranches}:\")\n",
    "#     model_ppo.learn(config[\"total_timesteps\"]//num_tranches, tb_log_name=\"PPO\", callback=ppo_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
    "#     model_ppo.save(f\"{config['savedir']}/{config['policy']}_PPO_step{i * (config['total_timesteps']//num_tranches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcNl-vXgmHgo"
   },
   "source": [
    "### Train SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294,
     "referenced_widgets": [
      "2bdf01d2ef434f29a948a4940625f6b0",
      "3bc1dfa18bf14d2a97852ff75ecad0f9"
     ]
    },
    "id": "6WOtrw-xmGhc",
    "outputId": "e0e86910-3357-457d-c95a-9bb833c3ae35"
   },
   "outputs": [],
   "source": [
    "# Instantiate and wrap the env\n",
    "env_sac = make_vec_env(SimpleSimGym, \n",
    "                   n_envs=1, \n",
    "                   monitor_dir=config[\"logdir\"]+\"sac\", \n",
    "                   env_kwargs=dict(\n",
    "                       max_budget=MAX_BUDGET, \n",
    "                       max_targets=MAX_TARGETS, \n",
    "                       num_classes=NUM_CLASSES, \n",
    "                       player_fov=PLAYER_FOV, \n",
    "                       render_mode=RENDER_MODE, \n",
    "                       action_format=ACTION_FORMAT))\n",
    "\n",
    "# Setup callbacks\n",
    "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=config[\"logdir\"], save_dir=config[\"savedir\"], model_name=\"sac\", verbose=0)\n",
    "\n",
    "# # Load the model from checkpoint 6M\n",
    "# model_sac = SAC.load(f\"{config['savedir']}/MlpPolicy_SAC_step8000000\", env_sac)\n",
    "# Create the agent\n",
    "model_sac = SAC(config[\"policy\"], env_sac, tensorboard_log=config[\"logdir\"], verbose=0)\n",
    "\n",
    "# # # Train the agent\n",
    "# # model_sac.learn(config[\"total_timesteps\"], tb_log_name=\"SAC\", callback=auto_save_callback, progress_bar=True)\n",
    "# # model_sac.save(f\"{config['savedir']}/{config['policy']}_SAC\")\n",
    "\n",
    "# Train in tranches\n",
    "times_trained = 0\n",
    "num_tranches = 10\n",
    "tranches = num_tranches*(1+times_trained)\n",
    "\n",
    "for i in range((num_tranches * times_trained)+1, tranches+1):\n",
    "    print(f\"^ Tranch {i}/{tranches}\")\n",
    "    if i == 1:\n",
    "        model_sac.learn(config[\"total_timesteps\"]//tranches, tb_log_name=\"SAC\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=True)\n",
    "    else:\n",
    "        model_sac.learn(config[\"total_timesteps\"]//tranches, tb_log_name=\"SAC\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
    "    model_sac.save(f\"{config['savedir']}/{config['policy']}_SAC_step{i * (config['total_timesteps']//tranches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w9ifee98kOL"
   },
   "source": [
    "### Continue Training or Run Dupliacte Experiments?\n",
    "\n",
    "This cell can be used to either continue training on an existing model (use `reset_num_timesteps=False`) or to run additional duplicate experiments training from scratch to test training consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sac.learn(config[\"total_timesteps\"], tb_log_name=\"SAC\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
    "# model_sac.save(f\"{config['savedir']}/{config['policy']}_SAC_pt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNtnuSaSAM8B"
   },
   "source": [
    "### Load Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yrtqc8GcAOT_"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "# model_sac = SAC.load(f\"{config['savedir']}/MlpPolicy_SAC_step4000000\")\n",
    "# model_ppo = PPO.load(f\"{config['savedir']}/MlpPolicy_PPO_step4000000\")\n",
    "\n",
    "best_sac = SAC.load(f\"{config['savedir']}/best_sac\")\n",
    "# best_ppo = PPO.load(f\"{config['savedir']}/best_ppo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBOWRrb_wmyx"
   },
   "source": [
    "### Check Performance\n",
    "\n",
    "Check if the policy can consistently succeed in the environment over multilpe episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBLWeFCdmGEE"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Instantiate the eval env\n",
    "eval_env = make_vec_env(SimpleSimGym, \n",
    "                   n_envs=1, \n",
    "                   # monitor_dir=config[\"logdir\"], \n",
    "                   env_kwargs=dict(\n",
    "                       max_budget=MAX_BUDGET, \n",
    "                       max_targets=MAX_TARGETS, \n",
    "                       num_classes=NUM_CLASSES, \n",
    "                       player_fov=PLAYER_FOV, \n",
    "                       render_mode=RENDER_MODE, \n",
    "                       action_format=ACTION_FORMAT\n",
    "                   )\n",
    "                  )\n",
    "\n",
    "# Check performance of best vs last model\n",
    "models = {\"last_sac\": model_sac, \"best_sac\": best_sac}#, \"last_ppo\": model_ppo, \"best_ppo\": best_ppo}\n",
    "# models = {\"best_sac\": best_sac, \"best_ppo\": best_ppo}\n",
    "\n",
    "for key in models.keys():\n",
    "    # Reset the eval env\n",
    "    eval_env.reset()\n",
    "    # Test average reward over multiple episodes\n",
    "    mean_reward, std_reward = evaluate_policy(models[key], eval_env, n_eval_episodes=50)\n",
    "    print(f\"MODEL TYPE: {key}\")\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx_YDEcGvjxR"
   },
   "source": [
    "### Visualize Trained Agent with Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY9ncXp8vjxR"
   },
   "outputs": [],
   "source": [
    "from utils import record_video, show_videos\n",
    "\n",
    "record_video(eval_env, model_sac, video_length=500*3, prefix=\"jupyter-sac-last-simplesim\")\n",
    "show_videos(\"videos\", prefix=\"jupyter-sac-last\")\n",
    "\n",
    "record_video(eval_env, best_sac, video_length=500*3, prefix=\"jupyter-sac-best-simplesim\")\n",
    "show_videos(\"videos\", prefix=\"jupyter-sac-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_video(eval_env, model_ppo, video_length=500*3, prefix=\"jupyter-ppo-last-simplesim\")\n",
    "# show_videos(\"videos\", prefix=\"jupyter-ppo-last\")\n",
    "\n",
    "# record_video(eval_env, best_ppo, video_length=500*3, prefix=\"jupyter-ppo-best-simplesim\")\n",
    "# show_videos(\"videos\", prefix=\"jupyter-ppo-best\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bdf01d2ef434f29a948a4940625f6b0": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_3bc1dfa18bf14d2a97852ff75ecad0f9",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">199,935/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:03:55</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">675 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m199,935/200,000 \u001b[0m [ \u001b[33m0:03:55\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m675 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "3bc1dfa18bf14d2a97852ff75ecad0f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af9946047ab64b4d9d91550f4f271c32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e14c6d372ee347f7be51d1027de08037": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_af9946047ab64b4d9d91550f4f271c32",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   5%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">10,217/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:25</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:07:24</span> , <span style=\"color: #800000; text-decoration-color: #800000\">428 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[35m   5%\u001b[0m \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10,217/200,000 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:07:24\u001b[0m , \u001b[31m428 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
