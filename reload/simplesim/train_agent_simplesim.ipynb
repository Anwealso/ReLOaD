{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoxOjIlOImwx"
   },
   "source": [
    "# SimpleSim Non-Holonomic Navigation Challenge\n",
    "\n",
    "This notebook attempts to train an agent solve a simplesim non-holonomic driving navigation problem with 1 target with random spawn location.\n",
    "This time, however we are teaching the agent to dwell at the goal as well instead of simply ending the episode.\n",
    "\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sp8rSS4DIhEV",
    "outputId": "5c0b7f03-662c-41c5-b3f9-f470e27754e1"
   },
   "outputs": [],
   "source": [
    "# !pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0wXGn1ylGFR"
   },
   "source": [
    "### Setup Tensorboard Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2BNcPi7lID_"
   },
   "outputs": [],
   "source": [
    "# # Clear any logs from previous runs\n",
    "# !rm -rf ./logs/\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqxatIwPOXe_"
   },
   "source": [
    "##  Custom Gym Envs\n",
    "\n",
    "Below are a couple of simpler lower order gridworld type gym environments that can be used as testing and debugging examples ,as well as our main SimpleSIm non-holonomic driving environment (which is imported from the seperate source files env.py and env_gym.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SNFv8TF58R0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYzDXA9vJfz1",
    "outputId": "f562a0de-9a0d-486e-fd16-06851997d502"
   },
   "outputs": [],
   "source": [
    "# Import our main environment\n",
    "from env_gym import SimpleSimGym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy5mlho1-Ine"
   },
   "source": [
    "### Validate the environment\n",
    "\n",
    "Stable Baselines3 provides a [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kH-Hxz7H53hl",
    "outputId": "92fb1484-c081-46bb-a0a3-f67e4f491d93"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CcUVatq-P0l",
    "outputId": "bb62c83a-ab64-49d4-ae7a-a367a4f2f23a"
   },
   "outputs": [],
   "source": [
    "env = SimpleSimGym(max_budget=500, max_targets=3, num_classes=10, player_fov=60)\n",
    "\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adsKMvDkRUn0"
   },
   "source": [
    "## Setup Callbacks\n",
    "\n",
    "### Auto Saving of the Best Model Callback\n",
    "\n",
    "Using the monitoring wrapper, we can save statistics of the environment, and use them to determine the mean training reward. This allows us to save the best model while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_2jApX051gi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzMHj7r3h78m"
   },
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq, log_dir, save_dir, model_name, verbose=1):\n",
    "    # def __init__(self, check_freq, log_dir, save_dir, filename, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir + model_name\n",
    "        self.save_path = os.path.join(save_dir, f\"best_{model_name}\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.log_dir is not None:\n",
    "            os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                    print(\n",
    "                        \"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(\n",
    "                            self.best_mean_reward, mean_reward\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model at {x[-1]} timesteps (saved to {self.save_path})\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ3khFtkSE0g"
   },
   "source": [
    "### Baseline Testing the Environment\n",
    "\n",
    "Test the performance of an untrained (random) policy on the environment so that we can get a baseline performance to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i62yf2LvSAYY"
   },
   "outputs": [],
   "source": [
    "# obs, _ = env.reset()\n",
    "# env.render()\n",
    "\n",
    "# print(env.observation_space)\n",
    "# print(env.action_space)\n",
    "# print(env.action_space.sample())\n",
    "\n",
    "#   for step in range(n_steps):\n",
    "#       env.observation_space.sample()\n",
    "#       obs, reward, terminated, truncated, info = env.step(action)\n",
    "#       done = terminated or truncated\n",
    "#       print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "#       env.render()\n",
    "#       # Then, go down\n",
    "#       print(f\"Step {step + 1}\")\n",
    "#       obs, reward, terminated, truncated, info = env.step(GO_DOWN)\n",
    "#       done = terminated or truncated\n",
    "#       print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "#       env.render()\n",
    "\n",
    "#       if done:\n",
    "#           print(\"Goal reached!\", \"reward=\", reward)\n",
    "#           break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv1e1qJETfHU"
   },
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fbrQWC8G103"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, SAC, TD3\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Environment Parameters\n",
    "MAX_BUDGET = 400\n",
    "MAX_TARGETS = 5\n",
    "NUM_CLASSES = 10\n",
    "PLAYER_FOV = 30\n",
    "RENDER_MODE = \"rgb_array\"\n",
    "ACTION_FORMAT = \"continuous\"\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MlpPolicy',\n",
    "    \"total_timesteps\": 2_000_000,\n",
    "    \"logdir\": \"logs/\",\n",
    "    \"savedir\": \"saved_models/\",\n",
    "}\n",
    "\n",
    "# Create log dir\n",
    "os.makedirs(config[\"logdir\"], exist_ok=True)\n",
    "\n",
    "# Create save dir\n",
    "os.makedirs(config[\"savedir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gmb5J448ly20"
   },
   "source": [
    "# Show Tensorboard Logs\n",
    "Visualise the live logs on tensorboard as we train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Tensorboard Logging\n",
    "%tensorboard --logdir logs/ --reload_interval 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcNl-vXgmHgo"
   },
   "source": [
    "### Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89,
     "referenced_widgets": [
      "e14c6d372ee347f7be51d1027de08037",
      "af9946047ab64b4d9d91550f4f271c32"
     ]
    },
    "id": "kBTmEWCf8jlF",
    "outputId": "51004445-a01e-4697-96f2-9a2c94d0e934"
   },
   "outputs": [],
   "source": [
    "# # Instantiate and wrap the env\n",
    "# env_ppo = make_vec_env(SimpleSimGym, \n",
    "#                    n_envs=1, \n",
    "#                    monitor_dir=config[\"logdir\"], \n",
    "#                    env_kwargs=dict(\n",
    "#                        max_budget=MAX_BUDGET, \n",
    "#                        max_targets=MAX_TARGETS, \n",
    "#                        num_classes=NUM_CLASSES, \n",
    "#                        player_fov=PLAYER_FOV, \n",
    "#                        render_mode=RENDER_MODE, \n",
    "#                        action_format=ACTION_FORMAT))\n",
    "\n",
    "# # Setup callbacks\n",
    "# ppo_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=config[\"logdir\"], save_dir=config[\"savedir\"], verbose=0)\n",
    "# # ppo_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=config[\"logdir\"], save_dir=config[\"savedir\"], model_name=\"ppo\", verbose=0)\n",
    "\n",
    "# # # Load the model from checkpoint 2\n",
    "# # model_ppo = PPO.load(f\"{config['savedir']}/MlpPolicy_PPO_step2000000\", env_ppo)\n",
    "\n",
    "# # Create the agent\n",
    "# model_ppo = PPO(config[\"policy\"], env_ppo, tensorboard_log=config[\"logdir\"], verbose=0)\n",
    "\n",
    "# # # Train the agent\n",
    "# # model_ppo.learn(config[\"total_timesteps\"], tb_log_name=\"PPO\", callback=ppo_save_callback, progress_bar=True)\n",
    "# # model_ppo.save(f\"{config['savedir']}/{config['policy']}_PPO\")\n",
    "\n",
    "# # Train in tranches\n",
    "# num_tranches = 10\n",
    "# for i in range(1, num_tranches+1):\n",
    "#     print(f\"Tranch {i}/{num_tranches}:\")\n",
    "#     model_ppo.learn(config[\"total_timesteps\"]//num_tranches, tb_log_name=\"PPO\", callback=ppo_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
    "#     model_ppo.save(f\"{config['savedir']}/{config['policy']}_PPO_step{i * (config['total_timesteps']//num_tranches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcNl-vXgmHgo"
   },
   "source": [
    "### Train SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294,
     "referenced_widgets": [
      "2bdf01d2ef434f29a948a4940625f6b0",
      "3bc1dfa18bf14d2a97852ff75ecad0f9"
     ]
    },
    "id": "6WOtrw-xmGhc",
    "outputId": "e0e86910-3357-457d-c95a-9bb833c3ae35",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate and wrap the env\n",
    "env_sac = make_vec_env(SimpleSimGym, \n",
    "                   n_envs=1, \n",
    "                   monitor_dir=config[\"logdir\"]+\"sac\", \n",
    "                   env_kwargs=dict(\n",
    "                       max_budget=MAX_BUDGET, \n",
    "                       max_targets=MAX_TARGETS, \n",
    "                       num_classes=NUM_CLASSES, \n",
    "                       player_fov=PLAYER_FOV, \n",
    "                       render_mode=RENDER_MODE, \n",
    "                       action_format=ACTION_FORMAT))\n",
    "\n",
    "# Setup callbacks\n",
    "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=config[\"logdir\"], save_dir=config[\"savedir\"], model_name=\"sac\", verbose=0)\n",
    "\n",
    "# # Load the model from checkpoint 2\n",
    "# model_sac = SAC.load(f\"{config['savedir']}/MlpPolicy_SAC_step800000\", env)\n",
    "\n",
    "# Create the agent\n",
    "model_sac = SAC(config[\"policy\"], env_sac, tensorboard_log=config[\"logdir\"], verbose=0)\n",
    "\n",
    "# # # Train the agent\n",
    "# # model_sac.learn(config[\"total_timesteps\"], tb_log_name=\"SAC\", callback=auto_save_callback, progress_bar=True)\n",
    "# # model_sac.save(f\"{config['savedir']}/{config['policy']}_SAC\")\n",
    "\n",
    "# Train in tranches\n",
    "num_tranches = 10\n",
    "for i in range(1, num_tranches+1):\n",
    "    print(f\"^ Tranch {i}/{num_tranches}\")\n",
    "    model_sac.learn(config[\"total_timesteps\"]//num_tranches, tb_log_name=\"SAC\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
    "    model_sac.save(f\"{config['savedir']}/{config['policy']}_SAC_step{i * (config['total_timesteps']//num_tranches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w9ifee98kOL"
   },
   "source": [
    "### Continue Training or Run Dupliacte Experiments?\n",
    "\n",
    "This cell can be used to either continue training on an existing model (use `reset_num_timesteps=False`) or to run additional duplicate experiments training from scratch to test training consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sac.learn(config[\"total_timesteps\"], tb_log_name=\"SAC\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
    "# model_sac.save(f\"{config['savedir']}/{config['policy']}_SAC_pt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNtnuSaSAM8B"
   },
   "source": [
    "### Load Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yrtqc8GcAOT_"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "# model_sac = SAC.load(f\"{config['savedir']}/MlpPolicy_SAC_step4000000\")\n",
    "# model_ppo = PPO.load(f\"{config['savedir']}/MlpPolicy_PPO_step4000000\")\n",
    "\n",
    "best_sac = SAC.load(f\"{config['savedir']}/best_sac\")\n",
    "# best_ppo = PPO.load(f\"{config['savedir']}/best_ppo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBOWRrb_wmyx"
   },
   "source": [
    "### Check Performance\n",
    "\n",
    "Check if the policy can consistently succeed in the environment over multilpe episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBLWeFCdmGEE"
   },
   "outputs": [],
   "source": [
    "# Instantiate the eval env\n",
    "eval_env = make_vec_env(SimpleSimGym, \n",
    "                   n_envs=1, \n",
    "                   monitor_dir=config[\"logdir\"], \n",
    "                   env_kwargs=dict(\n",
    "                       max_budget=MAX_BUDGET, \n",
    "                       max_targets=MAX_TARGETS, \n",
    "                       num_classes=NUM_CLASSES, \n",
    "                       player_fov=PLAYER_FOV, \n",
    "                       render_mode=RENDER_MODE, \n",
    "                       action_format=ACTION_FORMAT\n",
    "                   )\n",
    "                  )\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Check performance of best vs last model\n",
    "models = {\"last_sac\": model_sac, \"best_sac\": best_sac}#, \"last_ppo\": model_ppo, \"best_ppo\": best_ppo}\n",
    "# models = {\"best_sac\": best_sac, \"best_ppo\": best_ppo}\n",
    "\n",
    "for key in models.keys():\n",
    "    # Reset the eval env\n",
    "    eval_env.reset()\n",
    "    # Test average reward over multiple episodes\n",
    "    mean_reward, std_reward = evaluate_policy(models[key], eval_env, n_eval_episodes=50)\n",
    "    print(f\"MODEL TYPE: {key}\")\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFMA7kYCvjxQ"
   },
   "source": [
    "### Prepare Video Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmbgVRvIvjxR"
   },
   "outputs": [],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'\n",
    "\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "def show_videos(video_path=\"\", prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "    :param video_path: (str) Path to the folder containing videos\n",
    "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append(\n",
    "            \"\"\"<video alt=\"{}\" autoplay\n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>\"\"\".format(\n",
    "                mp4, video_b64.decode(\"ascii\")\n",
    "            )\n",
    "        )\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder #, DummyVecEnv\n",
    "\n",
    "# # Create videos dir\n",
    "# videos_dir = \"./videos/\"\n",
    "# os.makedirs(videos_dir, exist_ok=True)\n",
    "\n",
    "def record_video(eval_env, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
    "    \"\"\"\n",
    "    :param eval_env: (vec env)\n",
    "    :param model: (RL model)\n",
    "    :param video_length: (int)\n",
    "    :param prefix: (str)\n",
    "    :param video_folder: (str)\n",
    "    \"\"\"\n",
    "    # eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "    # Start the video at step=0 and record 500 steps\n",
    "    eval_env = VecVideoRecorder(\n",
    "        eval_env,\n",
    "        video_folder=video_folder,\n",
    "        record_video_trigger=lambda step: step == 0,\n",
    "        video_length=video_length,\n",
    "        name_prefix=prefix,\n",
    "    )\n",
    "\n",
    "    obs = eval_env.reset()\n",
    "    \n",
    "    for _ in range(video_length):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "    # Close the video recorder\n",
    "    eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx_YDEcGvjxR"
   },
   "source": [
    "### Visualize Trained Agent with Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY9ncXp8vjxR"
   },
   "outputs": [],
   "source": [
    "record_video(eval_env, model_sac, video_length=500*3, prefix=\"sac-last-simplesim\")\n",
    "show_videos(\"videos\", prefix=\"sac-last\")\n",
    "\n",
    "record_video(eval_env, best_sac, video_length=500*3, prefix=\"sac-best-simplesim\")\n",
    "show_videos(\"videos\", prefix=\"sac-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_video(eval_env, model_ppo, video_length=500*3, prefix=\"ppo-last-simplesim\")\n",
    "# show_videos(\"videos\", prefix=\"ppo-last\")\n",
    "\n",
    "# record_video(eval_env, best_ppo, video_length=500*3, prefix=\"ppo-best-simplesim\")\n",
    "# show_videos(\"videos\", prefix=\"ppo-best\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bdf01d2ef434f29a948a4940625f6b0": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_3bc1dfa18bf14d2a97852ff75ecad0f9",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">199,935/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:03:55</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">675 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m199,935/200,000 \u001b[0m [ \u001b[33m0:03:55\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m675 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "3bc1dfa18bf14d2a97852ff75ecad0f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af9946047ab64b4d9d91550f4f271c32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e14c6d372ee347f7be51d1027de08037": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_af9946047ab64b4d9d91550f4f271c32",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   5%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">10,217/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:25</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:07:24</span> , <span style=\"color: #800000; text-decoration-color: #800000\">428 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[35m   5%\u001b[0m \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10,217/200,000 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:07:24\u001b[0m , \u001b[31m428 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
