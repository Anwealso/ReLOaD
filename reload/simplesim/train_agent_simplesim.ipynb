{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoxOjIlOImwx"
   },
   "source": [
    "# SimpleSim Non-Holonomic Navigation Challenge\n",
    "\n",
    "This notebook attempts to train an agent solve a simplesim non-holonomic driving navigation problem with 1 target with random spawn location.\n",
    "This time, however we are teaching the agent to dwell at the goal as well instead of simply ending the episode.\n",
    "\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sp8rSS4DIhEV",
    "outputId": "5c0b7f03-662c-41c5-b3f9-f470e27754e1"
   },
   "outputs": [],
   "source": [
    "# !pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0wXGn1ylGFR"
   },
   "source": [
    "### Setup Tensorboard Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X2BNcPi7lID_"
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqxatIwPOXe_"
   },
   "source": [
    "##  Custom Gym Envs\n",
    "\n",
    "Below are a couple of simpler lower order gridworld type gym environments that can be used as testing and debugging examples ,as well as our main SimpleSIm non-holonomic driving environment (which is imported from the seperate source files env.py and env_gym.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5SNFv8TF58R0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYzDXA9vJfz1",
    "outputId": "f562a0de-9a0d-486e-fd16-06851997d502"
   },
   "outputs": [],
   "source": [
    "class GoLeftEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left.\n",
    "    \"\"\"\n",
    "\n",
    "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    # Define constants for clearer code\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
    "        super(GoLeftEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Size of the 1D-grid\n",
    "        self.grid_size = grid_size\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = grid_size - 1\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions, we have two: left and right\n",
    "        n_actions = 2\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        # The observation will be the coordinate of the agent\n",
    "        # this can be described both by Discrete and Box space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=self.grid_size-1, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed, options=options)\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = self.grid_size - 1\n",
    "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "        return np.array([self.agent_pos]).astype(np.float32), {}  # empty info dict\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid action={action} which is not part of the action space\"\n",
    "            )\n",
    "\n",
    "        # Account for the boundaries of the grid\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size-1)\n",
    "\n",
    "        # Are we at the left of the grid?\n",
    "        terminated = bool(self.agent_pos == 0)\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "\n",
    "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "        reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        if self.render_mode == \"console\":\n",
    "            print(\".\" * self.agent_pos, end=\"\")\n",
    "            print(\"x\", end=\"\")\n",
    "            print(\".\" * ((self.grid_size - self.agent_pos)-1))\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class GoDownLeftEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left.\n",
    "    \"\"\"\n",
    "\n",
    "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    # Define constants for clearer code\n",
    "    NOTHING = 0\n",
    "    RIGHT = 1\n",
    "    UP = 2\n",
    "    LEFT = 3\n",
    "    DOWN = 4\n",
    "\n",
    "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
    "        super(GoDownLeftEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Size of the 1D-grid\n",
    "        self.grid_size = grid_size\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos_x = grid_size - 1\n",
    "        self.agent_pos_y = grid_size - 1\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions, we have two: left and right\n",
    "        n_actions = 5\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        # The observation will be the x and y coordinates of the agent\n",
    "        # this can be described both by Discrete and Box space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=self.grid_size-1, shape=(2,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed, options=options)\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos_x = self.grid_size - 1\n",
    "        self.agent_pos_y = self.grid_size - 1\n",
    "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "        return np.array([self.agent_pos_x, self.agent_pos_y]).astype(np.float32), {}  # empty info dict\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.NOTHING:\n",
    "          # Do nothing\n",
    "          pass\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos_x += 1\n",
    "        elif action == self.UP:\n",
    "            self.agent_pos_y += 1\n",
    "        elif action == self.LEFT:\n",
    "            self.agent_pos_x -= 1\n",
    "        elif action == self.DOWN:\n",
    "            self.agent_pos_y -= 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid action={action} which is not part of the action space\"\n",
    "            )\n",
    "\n",
    "        # Account for the boundaries of the grid\n",
    "        self.agent_pos_x = np.clip(self.agent_pos_x, 0, self.grid_size-1)\n",
    "        self.agent_pos_y = np.clip(self.agent_pos_y, 0, self.grid_size-1)\n",
    "\n",
    "        # Are we at the left of the grid?\n",
    "        terminated = bool(self.agent_pos_x == 0 and self.agent_pos_y == 0)\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "\n",
    "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "        reward = 1 if (self.agent_pos_x == 0 and self.agent_pos_y == 0) else 0\n",
    "\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos_x, self.agent_pos_y]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        if self.render_mode == \"console\":\n",
    "            for row in reversed(range(self.grid_size)):\n",
    "                print(\".\" * self.agent_pos_x, end=\"\")\n",
    "                if (row == self.agent_pos_y):\n",
    "                    print(\"x\", end=\"\")\n",
    "                else:\n",
    "                    print(\".\", end=\"\")\n",
    "                print(\".\" * ((self.grid_size - self.agent_pos_x)-1))\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Import our main environment\n",
    "from env_gym import SimpleSimGym\n",
    "\n",
    "# This simple toggle can be used to switch which environment we are training the notebook on\n",
    "env_mode = 2  # 0 for GoLeft, 1 for GoDownLeft, 2 for SimpleSimGym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy5mlho1-Ine"
   },
   "source": [
    "### Validate the environment\n",
    "\n",
    "Stable Baselines3 provides a [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kH-Hxz7H53hl",
    "outputId": "92fb1484-c081-46bb-a0a3-f67e4f491d93"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CcUVatq-P0l",
    "outputId": "bb62c83a-ab64-49d4-ae7a-a367a4f2f23a"
   },
   "outputs": [],
   "source": [
    "if env_mode == 0:\n",
    "  env = GoLeftEnv()\n",
    "elif env_mode == 1:\n",
    "  env = GoDownLeftEnv()\n",
    "else:\n",
    "  env = SimpleSimGym(starting_budget=500, num_targets=3, player_fov=60, render_mode=\"rgb_array\")\n",
    "\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adsKMvDkRUn0"
   },
   "source": [
    "## Setup Callbacks\n",
    "\n",
    "### Auto Saving of the Best Model Callback\n",
    "\n",
    "Using the monitoring wrapper, we can save statistics of the environment, and use them to determine the mean training reward. This allows us to save the best model while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "x_2jApX051gi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nzMHj7r3h78m"
   },
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq, log_dir, save_dir, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(save_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.log_dir is not None:\n",
    "            os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            # print(self.log_dir)\n",
    "            # print(load_results(self.log_dir))\n",
    "            # print(ts2xy(load_results(self.log_dir), \"timesteps\"))\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                    print(\n",
    "                        \"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(\n",
    "                            self.best_mean_reward, mean_reward\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    # if self.verbose > 0:\n",
    "                    print(\"Saving new best model at {} timesteps\".format(x[-1]))\n",
    "                    print(\"Saving new best model to {}.zip\".format(self.save_path))\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WndGNiRKIIJB"
   },
   "source": [
    "### Curriculum Learning Callback\n",
    "\n",
    "Create a callback to slowly move the locations of the targets further away from the robot to increase early reward feedback for fast learning, and then increase the complexity later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Y8Xe4S4IhvZ"
   },
   "source": [
    "### Adaptive Training Rate Callback\n",
    "\n",
    "Reduces the learning rate over time to reduce instability in the later stages of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hJ1h8P5zIhfX"
   },
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value: float):\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ3khFtkSE0g"
   },
   "source": [
    "### Baseline Testing the Environment\n",
    "\n",
    "Test the performance of an untrained (random) policy on the environment so that we can get a baseline performance to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i62yf2LvSAYY"
   },
   "outputs": [],
   "source": [
    "# if env_mode == 0:\n",
    "#   env = GoLeftEnv(grid_size=10)\n",
    "# elif env_mode == 1:\n",
    "#   env = GoDownLeftEnv(grid_size=10)\n",
    "# else:\n",
    "#   env = SimpleSimGym(starting_budget=STARTING_BUDGET, num_targets=NUM_TARGETS, player_fov=PLAYER_FOV, render_mode=None)\n",
    "\n",
    "# obs, _ = env.reset()\n",
    "# env.render()\n",
    "\n",
    "# print(env.observation_space)\n",
    "# print(env.action_space)\n",
    "# print(env.action_space.sample())\n",
    "\n",
    "# if env_mode == 0:\n",
    "#   GO_LEFT = 0\n",
    "#   # Hardcoded best agent: always go left!\n",
    "#   n_steps = 20\n",
    "#   for step in range(n_steps):\n",
    "#       print(f\"Step {step + 1}\")\n",
    "#       obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "#       done = terminated or truncated\n",
    "#       print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "#       env.render()\n",
    "#       if done:\n",
    "#           print(\"Goal reached!\", \"reward=\", reward)\n",
    "#           break\n",
    "# else:\n",
    "#   GO_LEFT = 3\n",
    "#   GO_DOWN = 4\n",
    "#   # Hardcoded best agent: always go left!\n",
    "#   n_steps = 20\n",
    "#   for step in range(n_steps):\n",
    "#       # Go left\n",
    "#       print(f\"Step {step + 1}\")\n",
    "#       obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "#       done = terminated or truncated\n",
    "#       print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "#       env.render()\n",
    "#       # Then, go down\n",
    "#       print(f\"Step {step + 1}\")\n",
    "#       obs, reward, terminated, truncated, info = env.step(GO_DOWN)\n",
    "#       done = terminated or truncated\n",
    "#       print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "#       env.render()\n",
    "\n",
    "#       if done:\n",
    "#           print(\"Goal reached!\", \"reward=\", reward)\n",
    "#           break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv1e1qJETfHU"
   },
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5fbrQWC8G103"
   },
   "outputs": [],
   "source": [
    "# Environment Parameters\n",
    "STARTING_BUDGET = 1000\n",
    "NUM_TARGETS = 6\n",
    "PLAYER_FOV = 30\n",
    "RENDER_MODE = \"rgb_array\"\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MlpPolicy',\n",
    "    \"total_timesteps\": 4_000_000,\n",
    "    \"logdir\": \"logs/\",\n",
    "    \"savedir\": \"saved_models/\"\n",
    "}\n",
    "# steps_per_curriculum = config[\"total_timesteps\"] // 20\n",
    "\n",
    "# Create log dir\n",
    "os.makedirs(config[\"logdir\"], exist_ok=True)\n",
    "\n",
    "# Create save dir\n",
    "os.makedirs(config[\"savedir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PQfLBE28SNDr"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate and wrap the env\n",
    "if env_mode == 0:\n",
    "  env = make_vec_env(GoLeftEnv, n_envs=1, monitor_dir=config[\"logdir\"], env_kwargs=dict(grid_size=10))\n",
    "elif env_mode == 1:\n",
    "  env = make_vec_env(GoDownLeftEnv, n_envs=1, monitor_dir=config[\"logdir\"], env_kwargs=dict(grid_size=10))\n",
    "else:\n",
    "  env = make_vec_env(SimpleSimGym, n_envs=1, monitor_dir=config[\"logdir\"], env_kwargs=dict(starting_budget=STARTING_BUDGET, num_targets=NUM_TARGETS, player_fov=PLAYER_FOV, render_mode=RENDER_MODE))\n",
    "\n",
    "\n",
    "# Setup callbacks\n",
    "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=config[\"logdir\"], save_dir=config[\"savedir\"], verbose=0)\n",
    "\n",
    "# Setup curriculum and learnig rate schedules\n",
    "# curriculum = np.linspace(0, 1, num=(int(config[\"total_timesteps\"]/steps_per_curriculum))) # increase from 5 to farthest corner distance\n",
    "# print(curriculum)\n",
    "# learning_rate = np.linspace(50, (1.5*500)//2, num=(config[\"total_timesteps\"]//steps_per_curriculum)) # increase from 5 to farthest corner distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gmb5J448ly20"
   },
   "source": [
    "# Show Tensorboard Logs\n",
    "Visualise the live logs on tensorboard as we train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3bdcbd58c835d994\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3bdcbd58c835d994\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open Tensorboard Logging\n",
    "%tensorboard --logdir logs/ --reload_interval 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcNl-vXgmHgo"
   },
   "source": [
    "### Train A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294,
     "referenced_widgets": [
      "2bdf01d2ef434f29a948a4940625f6b0",
      "3bc1dfa18bf14d2a97852ff75ecad0f9"
     ]
    },
    "id": "6WOtrw-xmGhc",
    "outputId": "e0e86910-3357-457d-c95a-9bb833c3ae35",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train the agent\n",
    "# model_a2c = A2C(config[\"policy\"], env, tensorboard_log=config[\"logdir\"], verbose=0)\n",
    "# model_a2c.learn(config[\"total_timesteps\"], tb_log_name=\"A2C\", callback=auto_save_callback, progress_bar=True)\n",
    "# model_a2c.save(f\"{config['savedir']}/{config['policy']}_A2C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcNl-vXgmHgo"
   },
   "source": [
    "### Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89,
     "referenced_widgets": [
      "e14c6d372ee347f7be51d1027de08037",
      "af9946047ab64b4d9d91550f4f271c32"
     ]
    },
    "id": "kBTmEWCf8jlF",
    "outputId": "51004445-a01e-4697-96f2-9a2c94d0e934",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cf6560652e4a24b8ebb61a17a34a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model at 1000 timesteps\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving new best model at 1000 timesteps\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to saved_models/best_model.zip\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving new best model to saved_models/best_model.zip\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run additional duplicate experiments from scratch to test training consistency\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_ppo \u001b[38;5;241m=\u001b[39m PPO(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m], env, tensorboard_log\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogdir\u001b[39m\u001b[38;5;124m\"\u001b[39m], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel_ppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_timesteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_save_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model_ppo\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msavedir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_PPO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniforge3/envs/sb/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniforge3/envs/sb/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniforge3/envs/sb/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:166\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msde_sample_freq \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m n_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msde_sample_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Sample a new noise matrix\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset_noise(env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    169\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(obs_tensor)\n",
      "File \u001b[0;32m~/opt/miniforge3/envs/sb/lib/python3.10/site-packages/torch/autograd/grad_mode.py:55\u001b[0m, in \u001b[0;36mno_grad.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniforge3/envs/sb/lib/python3.10/site-packages/torch/autograd/grad_mode.py:149\u001b[0m, in \u001b[0;36mset_grad_enabled.__init__\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mset_grad_enabled\u001b[39;00m(_DecoratorContextManager):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Context-manager that sets gradient calculation on or off.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    ``set_grad_enabled`` will enable or disable grads based on its argument :attr:`mode`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m    151\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(mode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run additional duplicate experiments from scratch to test training consistency\n",
    "model_ppo = PPO(config[\"policy\"], env, tensorboard_log=config[\"logdir\"], verbose=0)\n",
    "model_ppo.learn(config[\"total_timesteps\"], tb_log_name=\"PPO\", callback=auto_save_callback, progress_bar=True)\n",
    "model_ppo.save(f\"{config['savedir']}/{config['policy']}_PPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIGZqeNXmJJK"
   },
   "source": [
    "### Curriculum Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OT_kKp2Ql_Js"
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# timesteps_trained = 0\n",
    "# while timesteps_trained < config[\"total_timesteps\"]:\n",
    "#     # model.learning_rate = learning_rate[i]\n",
    "#     env.set_attr(\"game.curriculum\", curriculum[i]) # up the env difficulty\n",
    "#     env.reset()\n",
    "#     print(f\"Step {timesteps_trained}: curriculum={env.get_attr('game.curriculum')}, learning_rate={model.learning_rate}\", flush=True)\n",
    "\n",
    "#     if i < 8:\n",
    "#       model.learn(steps_per_curriculum, tb_log_name=\"curriculum_learning\", progress_bar=True, reset_num_timesteps=False)\n",
    "#     else:\n",
    "#       # Only auto save from near the end otherwise we might get a policy only good at close targets\n",
    "#       model.learn(steps_per_curriculum, tb_log_name=\"curriculum_learning\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
    "\n",
    "#     timesteps_trained += steps_per_curriculum\n",
    "#     i += 1\n",
    "\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w9ifee98kOL"
   },
   "source": [
    "### Continue Training or Run Dupliacte Experiments?\n",
    "\n",
    "This cell can be used to either continue training on an existing model (use `reset_num_timesteps=False`) or to run additional duplicate experiments training from scratch to test training consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo.learn(2_000_000, tb_log_name=\"PPO\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
    "# model_ppo.save(f\"{config['savedir']}/{config['policy']}_PPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNtnuSaSAM8B"
   },
   "source": [
    "### Load Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yrtqc8GcAOT_"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = PPO.load(f\"{config['savedir']}/best_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaEjy8R3_JoP"
   },
   "source": [
    "### Save Model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9wH6IE__MTk"
   },
   "outputs": [],
   "source": [
    "# The model will be saved under MlpPolicy.zip\n",
    "# model.save(f\"{config['savedir']}/{config['policy']}_A2C\")\n",
    "# model2.save(f\"{config['savedir']}/{config['policy']}_PPO\")\n",
    "# model3.save(f\"{config['savedir']}/{config['policy']}3\")\n",
    "# model.save(f\"{config['savedir']}/{config['policy']}2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBOWRrb_wmyx"
   },
   "source": [
    "### Check Performance\n",
    "\n",
    "Check if the policy can consistently succeed in the environment over multilpe episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBLWeFCdmGEE"
   },
   "outputs": [],
   "source": [
    "# Instantiate the eval env\n",
    "if env_mode == 0:\n",
    "    eval_env = make_vec_env(GoLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))\n",
    "elif env_mode == 1:\n",
    "    eval_env = make_vec_env(GoDownLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))\n",
    "else:\n",
    "    eval_env = make_vec_env(SimpleSimGym, n_envs=1, env_kwargs=dict(starting_budget=STARTING_BUDGET, num_targets=NUM_TARGETS, player_fov=PLAYER_FOV, render_mode=RENDER_MODE))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# # Check performance of best vs last model\n",
    "models = {\"last_ppo\": model_ppo, \"best\": best_model}\n",
    "\n",
    "for key in models.keys():\n",
    "    # Instantiate the eval env\n",
    "    if env_mode == 0:\n",
    "        eval_env = make_vec_env(GoLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))\n",
    "    elif env_mode == 1:\n",
    "        eval_env = make_vec_env(GoDownLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))\n",
    "    else:\n",
    "        eval_env = make_vec_env(SimpleSimGym, n_envs=1, env_kwargs=dict(starting_budget=STARTING_BUDGET, num_targets=NUM_TARGETS, player_fov=PLAYER_FOV, render_mode=RENDER_MODE))\n",
    "\n",
    "    # Test average reward over multiple episodes\n",
    "    mean_reward, std_reward = evaluate_policy(models[key], eval_env, n_eval_episodes=50)\n",
    "    print(f\"MODEL TYPE: {key}\")\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFMA7kYCvjxQ"
   },
   "source": [
    "### Prepare Video Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmbgVRvIvjxR"
   },
   "outputs": [],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'\n",
    "\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "def show_videos(video_path=\"\", prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "    :param video_path: (str) Path to the folder containing videos\n",
    "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append(\n",
    "            \"\"\"<video alt=\"{}\" autoplay\n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>\"\"\".format(\n",
    "                mp4, video_b64.decode(\"ascii\")\n",
    "            )\n",
    "        )\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder #, DummyVecEnv\n",
    "\n",
    "# # Create videos dir\n",
    "# videos_dir = \"./videos/\"\n",
    "# os.makedirs(videos_dir, exist_ok=True)\n",
    "\n",
    "def record_video(eval_env, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
    "    \"\"\"\n",
    "    :param eval_env: (vec env)\n",
    "    :param model: (RL model)\n",
    "    :param video_length: (int)\n",
    "    :param prefix: (str)\n",
    "    :param video_folder: (str)\n",
    "    \"\"\"\n",
    "    # eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "    # Start the video at step=0 and record 500 steps\n",
    "    eval_env = VecVideoRecorder(\n",
    "        eval_env,\n",
    "        video_folder=video_folder,\n",
    "        record_video_trigger=lambda step: step == 0,\n",
    "        video_length=video_length,\n",
    "        name_prefix=prefix,\n",
    "    )\n",
    "\n",
    "    obs = eval_env.reset()\n",
    "    \n",
    "    for _ in range(video_length):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "    # Close the video recorder\n",
    "    eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx_YDEcGvjxR"
   },
   "source": [
    "### Visualize Trained Agent with Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY9ncXp8vjxR"
   },
   "outputs": [],
   "source": [
    "# if env_mode == 2:\n",
    "# record_video(eval_env, model_a2c, video_length=500*3, prefix=\"a2c-simplesim-last\")\n",
    "\n",
    "record_video(eval_env, model_ppo, video_length=500*3, prefix=\"ppo-last-simplesim\")\n",
    "record_video(eval_env, best_model, video_length=500*3, prefix=\"ppo-best-simplesim\")\n",
    "\n",
    "# record_video(eval_env, best_model, video_length=500*3, prefix=\"best-simplesim\")\n",
    "\n",
    "# show_videos(\"videos\")\n",
    "# show_videos(\"videos\", prefix=\"a2c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_videos(\"videos\", prefix=\"a2c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_videos(\"videos\", prefix=\"ppo-last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_videos(\"videos\", prefix=\"ppo-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check Total Ep Reward\n",
    "\n",
    "# # View Env Specs\n",
    "# # utils.show_env_summary(py_env)\n",
    "# num_episodes = 10\n",
    "# obs = env.reset()\n",
    "\n",
    "# for i in range(num_episodes):\n",
    "#     done = False\n",
    "#     ep_reward = 0\n",
    "\n",
    "#     while not done:\n",
    "#         action, _states = model_a2c.predict(obs)\n",
    "#         # action, _states = best_model_a2c.predict(obs)\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         # env.render()\n",
    "#         # print(f\"Reward: {reward}, Observation: {obs}\\n\")\n",
    "\n",
    "#         ep_reward += reward\n",
    "\n",
    "#         if done:\n",
    "#             obs = env.reset()\n",
    "\n",
    "#     print(f\"Total Ep Reward: {ep_reward}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bdf01d2ef434f29a948a4940625f6b0": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_3bc1dfa18bf14d2a97852ff75ecad0f9",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">199,935/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:03:55</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">675 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m199,935/200,000 \u001b[0m [ \u001b[33m0:03:55\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m675 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "3bc1dfa18bf14d2a97852ff75ecad0f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af9946047ab64b4d9d91550f4f271c32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e14c6d372ee347f7be51d1027de08037": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_af9946047ab64b4d9d91550f4f271c32",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   5%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">10,217/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:25</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:07:24</span> , <span style=\"color: #800000; text-decoration-color: #800000\">428 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[35m   5%\u001b[0m \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10,217/200,000 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:07:24\u001b[0m , \u001b[31m428 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
