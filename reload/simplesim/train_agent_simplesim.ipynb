{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoxOjIlOImwx"
   },
   "source": [
    "# SimpleSim Non-Holonomic Navigation Challenge\n",
    "\n",
    "This notebook attempts to train an agent solve a simplesim non-holonomic driving navigation problem with 1 target with random spawn location.\n",
    "This time, however we are teaching the agent to dwell at the goal as well instead of simply ending the episode.\n",
    "\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sp8rSS4DIhEV",
    "outputId": "5c0b7f03-662c-41c5-b3f9-f470e27754e1"
   },
   "outputs": [],
   "source": [
    "# !pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0wXGn1ylGFR"
   },
   "source": [
    "### Setup Tensorboard Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X2BNcPi7lID_"
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqxatIwPOXe_"
   },
   "source": [
    "##  Custom Gym Envs\n",
    "\n",
    "Below are a couple of simpler lower order gridworld type gym environments that can be used as testing and debugging examples ,as well as our main SimpleSIm non-holonomic driving environment (which is imported from the seperate source files env.py and env_gym.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5SNFv8TF58R0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYzDXA9vJfz1",
    "outputId": "f562a0de-9a0d-486e-fd16-06851997d502"
   },
   "outputs": [],
   "source": [
    "class GoLeftEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left.\n",
    "    \"\"\"\n",
    "\n",
    "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    # Define constants for clearer code\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
    "        super(GoLeftEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Size of the 1D-grid\n",
    "        self.grid_size = grid_size\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = grid_size - 1\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions, we have two: left and right\n",
    "        n_actions = 2\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        # The observation will be the coordinate of the agent\n",
    "        # this can be described both by Discrete and Box space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=self.grid_size-1, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed, options=options)\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = self.grid_size - 1\n",
    "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "        return np.array([self.agent_pos]).astype(np.float32), {}  # empty info dict\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid action={action} which is not part of the action space\"\n",
    "            )\n",
    "\n",
    "        # Account for the boundaries of the grid\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size-1)\n",
    "\n",
    "        # Are we at the left of the grid?\n",
    "        terminated = bool(self.agent_pos == 0)\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "\n",
    "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "        reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        if self.render_mode == \"console\":\n",
    "            print(\".\" * self.agent_pos, end=\"\")\n",
    "            print(\"x\", end=\"\")\n",
    "            print(\".\" * ((self.grid_size - self.agent_pos)-1))\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class GoDownLeftEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left.\n",
    "    \"\"\"\n",
    "\n",
    "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    # Define constants for clearer code\n",
    "    NOTHING = 0\n",
    "    RIGHT = 1\n",
    "    UP = 2\n",
    "    LEFT = 3\n",
    "    DOWN = 4\n",
    "\n",
    "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
    "        super(GoDownLeftEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Size of the 1D-grid\n",
    "        self.grid_size = grid_size\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos_x = grid_size - 1\n",
    "        self.agent_pos_y = grid_size - 1\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions, we have two: left and right\n",
    "        n_actions = 5\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        # The observation will be the x and y coordinates of the agent\n",
    "        # this can be described both by Discrete and Box space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=self.grid_size-1, shape=(2,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed, options=options)\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos_x = self.grid_size - 1\n",
    "        self.agent_pos_y = self.grid_size - 1\n",
    "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "        return np.array([self.agent_pos_x, self.agent_pos_y]).astype(np.float32), {}  # empty info dict\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.NOTHING:\n",
    "          # Do nothing\n",
    "          pass\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos_x += 1\n",
    "        elif action == self.UP:\n",
    "            self.agent_pos_y += 1\n",
    "        elif action == self.LEFT:\n",
    "            self.agent_pos_x -= 1\n",
    "        elif action == self.DOWN:\n",
    "            self.agent_pos_y -= 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid action={action} which is not part of the action space\"\n",
    "            )\n",
    "\n",
    "        # Account for the boundaries of the grid\n",
    "        self.agent_pos_x = np.clip(self.agent_pos_x, 0, self.grid_size-1)\n",
    "        self.agent_pos_y = np.clip(self.agent_pos_y, 0, self.grid_size-1)\n",
    "\n",
    "        # Are we at the left of the grid?\n",
    "        terminated = bool(self.agent_pos_x == 0 and self.agent_pos_y == 0)\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "\n",
    "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "        reward = 1 if (self.agent_pos_x == 0 and self.agent_pos_y == 0) else 0\n",
    "\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos_x, self.agent_pos_y]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        if self.render_mode == \"console\":\n",
    "            for row in reversed(range(self.grid_size)):\n",
    "                print(\".\" * self.agent_pos_x, end=\"\")\n",
    "                if (row == self.agent_pos_y):\n",
    "                    print(\"x\", end=\"\")\n",
    "                else:\n",
    "                    print(\".\", end=\"\")\n",
    "                print(\".\" * ((self.grid_size - self.agent_pos_x)-1))\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Import our main environment\n",
    "from env_gym import SimpleSimGym\n",
    "\n",
    "# This simple toggle can be used to switch which environment we are training the notebook on\n",
    "env_mode = 2  # 0 for GoLeft, 1 for GoDownLeft, 2 for SimpleSimGym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy5mlho1-Ine"
   },
   "source": [
    "### Validate the environment\n",
    "\n",
    "Stable Baselines3 provides a [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kH-Hxz7H53hl",
    "outputId": "92fb1484-c081-46bb-a0a3-f67e4f491d93"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CcUVatq-P0l",
    "outputId": "bb62c83a-ab64-49d4-ae7a-a367a4f2f23a"
   },
   "outputs": [],
   "source": [
    "if env_mode == 0:\n",
    "  env = GoLeftEnv()\n",
    "elif env_mode == 1:\n",
    "  env = GoDownLeftEnv()\n",
    "else:\n",
    "  env = SimpleSimGym(max_budget=500, max_targets=3, num_classes=10, player_fov=60)\n",
    "\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adsKMvDkRUn0"
   },
   "source": [
    "## Setup Callbacks\n",
    "\n",
    "### Auto Saving of the Best Model Callback\n",
    "\n",
    "Using the monitoring wrapper, we can save statistics of the environment, and use them to determine the mean training reward. This allows us to save the best model while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "x_2jApX051gi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nzMHj7r3h78m"
   },
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq, log_dir, save_dir, filename, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(save_dir, filename)\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.log_dir is not None:\n",
    "            os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            # print(self.log_dir)\n",
    "            # print(load_results(self.log_dir))\n",
    "            # print(ts2xy(load_results(self.log_dir), \"timesteps\"))\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                    print(\n",
    "                        \"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(\n",
    "                            self.best_mean_reward, mean_reward\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    # if self.verbose > 0:\n",
    "                    print(\"Saving new best model at {} timesteps\".format(x[-1]))\n",
    "                    print(\"Saving new best model to {}.zip\".format(self.save_path))\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WndGNiRKIIJB"
   },
   "source": [
    "### Curriculum Learning Callback\n",
    "\n",
    "Create a callback to slowly move the locations of the targets further away from the robot to increase early reward feedback for fast learning, and then increase the complexity later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Y8Xe4S4IhvZ"
   },
   "source": [
    "### Adaptive Training Rate Callback\n",
    "\n",
    "Reduces the learning rate over time to reduce instability in the later stages of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hJ1h8P5zIhfX"
   },
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value: float):\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ3khFtkSE0g"
   },
   "source": [
    "### Baseline Testing the Environment\n",
    "\n",
    "Test the performance of an untrained (random) policy on the environment so that we can get a baseline performance to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i62yf2LvSAYY"
   },
   "outputs": [],
   "source": [
    "# if env_mode == 0:\n",
    "#   env = GoLeftEnv(grid_size=10)\n",
    "# elif env_mode == 1:\n",
    "#   env = GoDownLeftEnv(grid_size=10)\n",
    "# else:\n",
    "#   env = SimpleSimGym(max_budget=MAX_BUDGET, max_targets=MAX_TARGETS, player_fov=PLAYER_FOV, render_mode=None)\n",
    "\n",
    "# obs, _ = env.reset()\n",
    "# env.render()\n",
    "\n",
    "# print(env.observation_space)\n",
    "# print(env.action_space)\n",
    "# print(env.action_space.sample())\n",
    "\n",
    "# if env_mode == 0:\n",
    "#   GO_LEFT = 0\n",
    "#   # Hardcoded best agent: always go left!\n",
    "#   n_steps = 20\n",
    "#   for step in range(n_steps):\n",
    "#       print(f\"Step {step + 1}\")\n",
    "#       obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "#       done = terminated or truncated\n",
    "#       print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "#       env.render()\n",
    "#       if done:\n",
    "#           print(\"Goal reached!\", \"reward=\", reward)\n",
    "#           break\n",
    "# else:\n",
    "#   GO_LEFT = 3\n",
    "#   GO_DOWN = 4\n",
    "#   # Hardcoded best agent: always go left!\n",
    "#   n_steps = 20\n",
    "#   for step in range(n_steps):\n",
    "#       # Go left\n",
    "#       print(f\"Step {step + 1}\")\n",
    "#       obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "#       done = terminated or truncated\n",
    "#       print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "#       env.render()\n",
    "#       # Then, go down\n",
    "#       print(f\"Step {step + 1}\")\n",
    "#       obs, reward, terminated, truncated, info = env.step(GO_DOWN)\n",
    "#       done = terminated or truncated\n",
    "#       print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "#       env.render()\n",
    "\n",
    "#       if done:\n",
    "#           print(\"Goal reached!\", \"reward=\", reward)\n",
    "#           break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv1e1qJETfHU"
   },
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5fbrQWC8G103"
   },
   "outputs": [],
   "source": [
    "# Environment Parameters\n",
    "MAX_BUDGET = 400\n",
    "MAX_TARGETS = 5\n",
    "NUM_CLASSES = 10\n",
    "PLAYER_FOV = 30\n",
    "RENDER_MODE = \"rgb_array\"\n",
    "ACTION_FORMAT = \"continuous\"\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MlpPolicy',\n",
    "    \"total_timesteps\": 4_000_000,\n",
    "    \"logdir\": \"logs/\",\n",
    "    \"savedir\": \"saved_models/\",\n",
    "}\n",
    "# steps_per_curriculum = config[\"total_timesteps\"] // 20\n",
    "\n",
    "# Create log dir\n",
    "os.makedirs(config[\"logdir\"], exist_ok=True)\n",
    "\n",
    "# Create save dir\n",
    "os.makedirs(config[\"savedir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PQfLBE28SNDr"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, SAC, TD3\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate and wrap the env\n",
    "if env_mode == 0:\n",
    "  env = make_vec_env(GoLeftEnv, n_envs=1, monitor_dir=config[\"logdir\"], env_kwargs=dict(grid_size=10))\n",
    "elif env_mode == 1:\n",
    "  env = make_vec_env(GoDownLeftEnv, n_envs=1, monitor_dir=config[\"logdir\"], env_kwargs=dict(grid_size=10))\n",
    "else:\n",
    "  env = make_vec_env(SimpleSimGym, n_envs=1, monitor_dir=config[\"logdir\"], env_kwargs=dict(max_budget=MAX_BUDGET, max_targets=MAX_TARGETS, num_classes=NUM_CLASSES, player_fov=PLAYER_FOV, render_mode=RENDER_MODE, action_format=ACTION_FORMAT))\n",
    "\n",
    "\n",
    "# # Setup callbacks\n",
    "# auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=config[\"logdir\"], save_dir=config[\"savedir\"], filename=\"best_model\", verbose=0)\n",
    "\n",
    "# Setup curriculum and learnig rate schedules\n",
    "# curriculum = np.linspace(0, 1, num=(int(config[\"total_timesteps\"]/steps_per_curriculum))) # increase from 5 to farthest corner distance\n",
    "# print(curriculum)\n",
    "# learning_rate = np.linspace(50, (1.5*500)//2, num=(config[\"total_timesteps\"]//steps_per_curriculum)) # increase from 5 to farthest corner distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gmb5J448ly20"
   },
   "source": [
    "# Show Tensorboard Logs\n",
    "Visualise the live logs on tensorboard as we train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11120), started 1:14:44 ago. (Use '!kill 11120' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-25029fd619e80ef2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-25029fd619e80ef2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open Tensorboard Logging\n",
    "%tensorboard --logdir logs/ --reload_interval 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcNl-vXgmHgo"
   },
   "source": [
    "### Train SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294,
     "referenced_widgets": [
      "2bdf01d2ef434f29a948a4940625f6b0",
      "3bc1dfa18bf14d2a97852ff75ecad0f9"
     ]
    },
    "id": "6WOtrw-xmGhc",
    "outputId": "e0e86910-3357-457d-c95a-9bb833c3ae35"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fed750ce5d146b0bd32dca32aac4cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model at 762 timesteps\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving new best model at 762 timesteps\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to saved_models/best_sac.zip\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving new best model to saved_models/best_sac.zip\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model at 1919 timesteps\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving new best model at 1919 timesteps\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to saved_models/best_sac.zip\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving new best model to saved_models/best_sac.zip\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model at 2713 timesteps\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving new best model at 2713 timesteps\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to saved_models/best_sac.zip\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving new best model to saved_models/best_sac.zip\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup callbacks\n",
    "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=config[\"logdir\"], save_dir=config[\"savedir\"], filename=\"best_sac\", verbose=0)\n",
    "\n",
    "# Create the agent\n",
    "model_sac = SAC(config[\"policy\"], env, tensorboard_log=config[\"logdir\"], verbose=0)\n",
    "\n",
    "# # Train the agent\n",
    "# model_sac.learn(config[\"total_timesteps\"], tb_log_name=\"SAC\", callback=auto_save_callback, progress_bar=True)\n",
    "# model_sac.save(f\"{config['savedir']}/{config['policy']}_SAC\")\n",
    "\n",
    "# Train in tranches\n",
    "num_tranches = 10\n",
    "for i in range(1, num_tranches+1):\n",
    "    model_sac.learn(config[\"total_timesteps\"]//num_tranches, tb_log_name=\"SAC\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
    "    model_sac.save(f\"{config['savedir']}/{config['policy']}_SAC_step{i * (config['total_timesteps']//num_tranches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcNl-vXgmHgo"
   },
   "source": [
    "### Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89,
     "referenced_widgets": [
      "e14c6d372ee347f7be51d1027de08037",
      "af9946047ab64b4d9d91550f4f271c32"
     ]
    },
    "id": "kBTmEWCf8jlF",
    "outputId": "51004445-a01e-4697-96f2-9a2c94d0e934",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=config[\"logdir\"], save_dir=config[\"savedir\"], filename=\"best_ppo\", verbose=0)\n",
    "\n",
    "# Create the agent\n",
    "model_ppo = PPO(config[\"policy\"], env, tensorboard_log=config[\"logdir\"], verbose=0)\n",
    "\n",
    "# # Train the agent\n",
    "# model_ppo.learn(config[\"total_timesteps\"], tb_log_name=\"PPO\", callback=auto_save_callback, progress_bar=True)\n",
    "# model_ppo.save(f\"{config['savedir']}/{config['policy']}_PPO\")\n",
    "\n",
    "# Train in tranches\n",
    "num_tranches = 10\n",
    "for i in range(1, num_tranches+1):\n",
    "    model_sac.learn(config[\"total_timesteps\"]//num_tranches, tb_log_name=\"PPO\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
    "    model_sac.save(f\"{config['savedir']}/{config['policy']}_PPO_step{i * (config['total_timesteps']//num_tranches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w9ifee98kOL"
   },
   "source": [
    "### Continue Training or Run Dupliacte Experiments?\n",
    "\n",
    "This cell can be used to either continue training on an existing model (use `reset_num_timesteps=False`) or to run additional duplicate experiments training from scratch to test training consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sac.learn(config[\"total_timesteps\"], tb_log_name=\"SAC\", callback=auto_save_callback, progress_bar=True, reset_num_timesteps=False)\n",
    "# model_sac.save(f\"{config['savedir']}/{config['policy']}_SAC_pt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNtnuSaSAM8B"
   },
   "source": [
    "### Load Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yrtqc8GcAOT_"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_sac = SAC.load(f\"{config['savedir']}/MlpPolicy_SAC_step4000000\")\n",
    "best_ppo = PPO.load(f\"{config['savedir']}/MlpPolicy_ppo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBOWRrb_wmyx"
   },
   "source": [
    "### Check Performance\n",
    "\n",
    "Check if the policy can consistently succeed in the environment over multilpe episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBLWeFCdmGEE"
   },
   "outputs": [],
   "source": [
    "## Instantiate the eval env\n",
    "if env_mode == 0:\n",
    "    eval_env = make_vec_env(GoLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))\n",
    "elif env_mode == 1:\n",
    "    eval_env = make_vec_env(GoDownLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))\n",
    "else:\n",
    "    eval_env = make_vec_env(SimpleSimGym, n_envs=1, env_kwargs=dict(max_budget=MAX_BUDGET, max_targets=MAX_TARGETS, num_classes=NUM_CLASSES, player_fov=PLAYER_FOV, render_mode=RENDER_MODE, action_format=ACTION_FORMAT))\n",
    "\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Check performance of best vs last model\n",
    "# models = {\"last_sac\": model_sac, \"best_sac\": best_sac, \"last_ppo\": model_ppo, \"best_ppo\": best_ppo}\n",
    "models = {\"best_sac\": best_sac, \"best_ppo\": best_ppo}\n",
    "\n",
    "for key in models.keys():\n",
    "    # Reset the eval env\n",
    "    eval_env.reset()\n",
    "    # Test average reward over multiple episodes\n",
    "    mean_reward, std_reward = evaluate_policy(models[key], eval_env, n_eval_episodes=50)\n",
    "    print(f\"MODEL TYPE: {key}\")\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFMA7kYCvjxQ"
   },
   "source": [
    "### Prepare Video Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmbgVRvIvjxR"
   },
   "outputs": [],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'\n",
    "\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "def show_videos(video_path=\"\", prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "    :param video_path: (str) Path to the folder containing videos\n",
    "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append(\n",
    "            \"\"\"<video alt=\"{}\" autoplay\n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>\"\"\".format(\n",
    "                mp4, video_b64.decode(\"ascii\")\n",
    "            )\n",
    "        )\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder #, DummyVecEnv\n",
    "\n",
    "# # Create videos dir\n",
    "# videos_dir = \"./videos/\"\n",
    "# os.makedirs(videos_dir, exist_ok=True)\n",
    "\n",
    "def record_video(eval_env, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
    "    \"\"\"\n",
    "    :param eval_env: (vec env)\n",
    "    :param model: (RL model)\n",
    "    :param video_length: (int)\n",
    "    :param prefix: (str)\n",
    "    :param video_folder: (str)\n",
    "    \"\"\"\n",
    "    # eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "    # Start the video at step=0 and record 500 steps\n",
    "    eval_env = VecVideoRecorder(\n",
    "        eval_env,\n",
    "        video_folder=video_folder,\n",
    "        record_video_trigger=lambda step: step == 0,\n",
    "        video_length=video_length,\n",
    "        name_prefix=prefix,\n",
    "    )\n",
    "\n",
    "    obs = eval_env.reset()\n",
    "    \n",
    "    for _ in range(video_length):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "    # Close the video recorder\n",
    "    eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx_YDEcGvjxR"
   },
   "source": [
    "### Visualize Trained Agent with Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY9ncXp8vjxR"
   },
   "outputs": [],
   "source": [
    "record_video(eval_env, model_sac, video_length=500*3, prefix=\"sac-last-simplesim\")\n",
    "record_video(eval_env, best_sac, video_length=500*3, prefix=\"sac-best-simplesim\")\n",
    "# show_videos(\"videos\", prefix=\"sac-last\")\n",
    "show_videos(\"videos\", prefix=\"sac-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_video(eval_env, model_ppo, video_length=500*3, prefix=\"ppo-last-simplesim\")\n",
    "record_video(eval_env, best_ppo, video_length=500*3, prefix=\"ppo-best-simplesim\")\n",
    "# show_videos(\"videos\", prefix=\"ppo-last\")\n",
    "show_videos(\"videos\", prefix=\"ppo-best\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bdf01d2ef434f29a948a4940625f6b0": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_3bc1dfa18bf14d2a97852ff75ecad0f9",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">199,935/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:03:55</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">675 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m199,935/200,000 \u001b[0m [ \u001b[33m0:03:55\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m675 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "3bc1dfa18bf14d2a97852ff75ecad0f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af9946047ab64b4d9d91550f4f271c32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e14c6d372ee347f7be51d1027de08037": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_af9946047ab64b4d9d91550f4f271c32",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   5%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">10,217/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:25</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:07:24</span> , <span style=\"color: #800000; text-decoration-color: #800000\">428 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[35m   5%\u001b[0m \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10,217/200,000 \u001b[0m [ \u001b[33m0:00:25\u001b[0m < \u001b[36m0:07:24\u001b[0m , \u001b[31m428 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
